\chapter{Zdroje dát potrebných pre analýzu}
\label{data-sources-available-for-research}

% \todo{Review and insert figures where appropriate}

\todo{GCP a Tranco Source Rankings finish }

Pre účely analýzy využívania technológie NEL je nutné nejakým spôsobom získať pohľad do diania vo verejnom internete.
Cieľom je pozerať sa na reálne komunikácie, ktoré buď už prebehli, alebo skúmať ako určité služby dostupné na internete
odpovedajú na HTTP(S) požiadavky zaslané v reálnom čase. V tejto kapitole sú do detailu rozvedené dostupné zdroje dát 
a ktoré z nich sú reálne aj použité v praktickej časti tejto práce -- kapitole \ref{analysis-and-its-results}.

\section{Potrebné dáta}

To čo pre analýzu v tejto práci je potrebné zaobstarať sú hlavne konkrétne zdroje dát, z ktorých možno čerpať. Čo sa týka priamo toho,
čo by mali obsahovať, v ideálnom prípade by išlo o presný výpis všetkých \textbf{webových domén dostupných verejne na internete} a prípadne aj ich subdomén, ktoré možno skúmať. 
Zoznam spĺňajúci takýto popis je potrebný práve preto aby sme jednak vedeli, ktoré domény je vôbec možné analyzovať a ďalej preto, aby sme vo výsledkoch práce vedeli s touto informáciou pracovať a spájať relevantné vzťahy ako napríklad medzi jednotlivou doménou a jej vlastníkom. 
Práve touto spojitosťou sa postupne môžeme dostať napríklad o prehľade o tom, kto vlastní najvyšší počet domén využívajúcich NEL. 
Takýchto zoznamov existuje hneď niekoľko a sú spomenuté v ďalšej kapitole (\ref{tranco}), kde sú do detailu popísané aj ich dôležité vlastnosti.

Následne, v bode, keď už by bolo známe, ktoré domény je možné podrobiť analýze, je nutné k nim nejako prideliť aj dáta týkajúce sa ich reálneho sieťového prenosu. 
Keďže je tu zameraním technológia NEL, pod sieťovým prenosom je myslený menovite protokol \textbf{HTTP(S)}, v ktorom majú byť preverené prítomnosti hlavičiek spojazdňujúcich NEL. Existuje celá sada metód, ako sa k tomuto prenosu dostať. 
Z hľadiska prítomnosti, a teda aktuálneho stavu webových technológií je vhodné použiť napríklad techniku takzvanú \textbf{Web Crawling}, ktorá je v kapitole \ref{web-crawling} popísaná v kontexte s využiteľnými zdrojmi dát, ktoré ju používajú.
Na účel Web Crawlingu je možné použiť už existujúce technológie ako \textbf{Selenium}. 
Selenium opisuje kapitola \ref{selenium}.
No aby bol nadobudnutý celkový prehľad, je nevyhnutné nahliadnuť taktiež do histórie prevádzky webu. 
V tomto bode je zasa nutné hľadať už spracované a uložené dáta získané či už Web Crawlingom, alebo inými spôsobmi. 
Vyhovujúcim sprostredkovávateľom takýchto historických dát je napríklad \textbf{HTTP Archive}, ktorému sa práca venuje primárne (\ref{httparchive}).
Ide o službu, ktorá zaznamenáva vývoj webu od roku 2010 a teda jej vhodnosť sa potvrdzuje tým, že samotná špecifikácia skúmanej technológie v tejto práci bola publikovaná až v roku 2018.

V oblasti zdrojov historických dát sa nachádzajú aj určité alternatívy, ktoré je možné využiť komplementárne k spomenutému primárnemu zdroju.
Im je tu venovaná iba okrajová pozornosť, no každá z nich ale za zmienku nepopierateľne stojí, a preto sú popísané v kapitole \ref{httparchive-alternatives}.


\section{TRANCO}
\label{tranco}

Ako už bolo spomenuté, vzniká potreba zaobstarania si zoznamu domén, s ktorými možno pracovať. Čo sa vlastností takýchto domén týka,
mali by vystavovať verejnému internetu službu, ktorá je dosiahnuteľná, komunikuje protokolom HTTP, a ideálne je aj často navštevovaná,
a tým pádom relevantná pre verejnosť. Najviac teda prichádza do úvahy zamerať sa na získanie zoznamu \textbf{web stránok}, zoradeného podľa
ich popularity (návštevnosti), ktorý bude zároveň nadobúdať rozumnú veľkosť pre účely prieskumu. 

Zaobstarávaniu zoznamov web stránok, ktoré majú spĺňať konkrétne predurčené kritéria sa venuje rad iných existujúcich prác/služieb 
\cite{tranco}\cite{hacker-target-website-lists-overview}. Táto práca namiesto implementovania vlastného riešenia využíva jedno z nich -- TRANCO.

TRANCO je rebríček webových stránok zoradených podľa hodnotenia ich populárnosti/navštevovanosti, ktorý je odolný proti externej manipulácií a vhodný na účely výskumu \cite{tranco-homepage}. 
Vznikol na základe častej potreby pre skúmanie práve takýchto stránok či už pre samotnú referenciu, alebo ako podklad pre ďalší prieskum.
Obsahuje primárne Pay-Level Domény (PLD), čo sú domény, ktoré si môže priamo zakúpiť či už jednotlivec alebo organizácia. PLD sa skladajú zo subdomény verejného 
suffixu\footnote{Verejný suffix (public suffix) je jedným z takých, pod ktorými je alebo historicky bolo možné registrovať doménové mená pre používateľov internetu. Ide napríklad o .com, .co.uk 
alebo aj pvt.k12.ma.us. Viď oficiálnu stránku so zoznamov verejných suffixov -- \href{https://publicsuffix.org/}{https://publicsuffix.org/}} 
alebo efektívnej Top-Level Domény (eTLD)\footnote{Efektívna top-level doména, pod ktorou môžu byť domény registrované jedinou organizáciu. eTLD je práve tá časť domény, ktorá naselduje za poslednoým 
znakom \code{\textbf{.}} -- napríklad pre \code{crookedtimber.org} je eTLD \code{\textbf{org}}. Viď dokumentáciu pre bližšie info -- \href{https://developer.mozilla.org/en-US/docs/Glossary/eTLD}
{https://developer.mozilla.org/en-US/docs/Glossary/eTLD}} -- napríklad \code{.com} ale aj \code{.co.uk} \cite{tranco}. Okrem PLD je možné vygenerovať rebríček aj so subdoménami (viď 
\ref{tranco-generation}).

Príkladom využitia TRANCO môže byť analýza využitia konkrétnych webových technológií na týchto stránkach -- od použitej metódy komprimácie jej obsahu po samotné rámce, pomocou ktorých je 
stránka vyvinutá. 
TRANCO nie je prvým takýmto rebríčkom, ale je prvým, ktorý sa zaoberá nedostatkami jeho \textbf{predchodcov} a \textbf{spája stránky uvedené v nich} do \textbf{jednotného rebríčka}, 
ktorý je stabilnejší, reprezentuje stránky v globálnej škále a dokonca do určitej miery odstraňuje stránky s potencionálne nebezpečným obsahom (napríklad phishing) \cite{tranco}. 

\subsection{Generovanie}
\label{tranco-generation}

V jeho štandardnej forme, rebríček TRANCO sa generuje každý deň v dvoch verziách:
\begin{itemize}
    \item TRANCO rebríček domén
    \item TRANCO rebríček domén a subdomén
\end{itemize}

V tomto každodenne generovanom rebríčku sú prednastavené ako zdroje dát (použité existujúce rebríčky), tak aj dátumový rozsah, za ktorý sa zo zdrojových rebríčkov má čerpať.
Štandardne sa teda vytvára výsledok z rebríčkov Alexa, Umbrella, Majestic a Quantcast, ktoré bližšie predstavuje nasledujúca kapitola (\ref{tranco-source-rankings}). 
Dátumový rozsah je nastavený na posledných 30 dní \cite{tranco-github}, pričom TRANCO použije ako kompletný zdroj dát všetky spomenuté rebríčky vygenerované za túto dobu. 

V novom TRANCO rebríčku sa zo zdrojových rebríčkov spriemeruje pre každú doménu jej zaradenie aplikovaním jednej z dostupných TRANCO kombinačných metód pre upravenie finálneho hodnotenia domén.
Pre štandardný rebríček je využitá kombinačná metóda takzvanej harmonickej progresie, zvaná \textbf{Dowdall rule}. Dowdall rule hodnotí v zozname obsahujúcom N domén prvú skórom 1 a všetky ostatné postupne \(1/2\), \(1/3\) až \(1/(N-1)\) a zakončí hodnotením poslednej -- \(1/N\) \cite{tranco}\cite{tranco-homepage}.

Po dokončení priemerovania a zaraďovania sa spolu s výsledným rebríčkom vytvorí na oficiálnom webe TRANCO aj jedinečná stránka obsahujúca odkaz na jeho stiahnutie a tiež citácia, 
ktorou je možné jedinečne referencovať tento nový rebríček v prácach, ktoré ho môžu použiť na svoje účely.

Zároveň, okrem generovania nových TRANCO rebríčkov sú na oficiálnej stránke dostupné aj historicky vygenerované, spomenuté \textbf{štandardné}, teda bežné, každodenné rebríčky \cite{tranco-homepage}.

\subsubsection{Možnosti konfigurácie}
\label{tranco-config}

Je taktiež možné vytvoriť žiadosť o vygenerovanie zoznamu s vlastnou konfiguráciou. 
To je možné uskutočniť na oficiálnej stránke TRANCO\footnote{\href{https://tranco-list.eu/configure}{https://tranco-list.eu/configure}},
kde si užívateľ môže vyskladať žiadanú konfiguráciu a vygenerovať podľa nej nový zoznam.
Konfigurovateľnosť nového zoznamu pozostáva z nasledujúcich možností: \cite{tranco-config}
\begin{enumerate}
    \item Zdrojové rebríčky domén
    
    Celkový výber možných vstupných rebríčkov je nasledovný -- Chrome User Experience Report, \textbf{Majestic}, Radar, Cisco Umbrella, \textbf{Alexa}, Quantcast, Farsight
    
    \item Počet dní, za ktoré zbierať vstupné rebríčky
    \begin{enumerate}
        \item od špecifikovaného počiatočného dátumu
        \item od špecifikovaného koncového dátumu
    \end{enumerate}

    \item Kombinačná metóda hodnotenia domén:
    \begin{enumerate}
        \item Aritmetická progresia -- \textbf{Borda count}\footnote{\href{https://courses.lumenlearning.com/waymakermath4libarts/chapter/borda-count}{https://courses.lumenlearning.com/waymakermath4libarts/chapter/borda-count (Borda count)}}
        \item Harmonická progresia -- \textbf{Dowdall rule} (štandardne zvolená)
    \end{enumerate}

    \item Počet prvých N domén, ktoré vziať ako vstup z každého rebríčka (štandardne milión)

    \item Možnosti filtrovania
    \begin{itemize}
        \item Podľa zaradenia domény v rámci zdrojových rebríčkov
        \begin{enumerate}
            \item nachádza sa v zoznamoch aspoň počas N dní
            \item nachádza sa aspoň v N zoznamoch
            \item nenachádza sa v zozname potencionálne nebezpečných domén\footnote{\href{https://safebrowsing.google.com}{https://safebrowsing.google.com (Google Safe Browsing)}}
        \end{enumerate}

        \item Podľa domény samotnej
        \begin{enumerate}
            \item pracovať iba s Pay-Level Doménami (PLD)
            \item pracovať s doménami podľa ich efektívnej Top Level Domény (eTLD):
            
            Užívateľ má možnosť definovať zoznam čiarkou oddelených eTLD, ktoré môžu byť buď ako jediné zahrnuté vo
            výsledku, alebo je ešte možnosť všetky definované odfiltrovať z výsledku preč
            
            \item pracovať iba s jednou doménou (najpopulárnejšou) pre každú nájdenú organizáciu (napríklad \code{google.com})
            \item pracovať iba s doménami, ktorých subdomény sa nachádzajú v zozname subdomén definovaným užívateľom 
        \end{enumerate}

        \item Podľa možností špecifických pre zoznam Chrome User Experience Report, a teda filtrovanie podľa krajiny, regiónu alebo podregiónu, do ktorého doména spadá. Užívateľ pri voľbe tohto filtru musí vyznačiť na predpripravenom zozname, ktoré krajiny, regióny a podregióny si želá zaradiť do výsledného rebríčka.
    \end{itemize}

    \item Ohraničenie výsledkov vo finálnom rebríčku (štandardne milión)
    
\end{enumerate}


\subsection{Zdrojové rebríčky domén}
\label{tranco-source-rankings}

Ako už bolo zmienené vyššie, stratégia získavania stránok, ktoré následne TRANCO podrobí hodnoteniu spočíva na vyťahovaní stránok z už existujúcich zoznamov s hotovým hodnotením. 
Zdrojom dát pre hodnotenie stránok TRANCO je teda množina niekoľkých podobných zoznamov, ktoré sami na ich úrovni používajú rôžne stratégie pre obstarávanie stránok a ich zoraďovanie 
podľa hodnotenia navštevovanosti \cite{tranco-methodology}. V tejto kapitole sú popísané zainteresované zoznamy, z ktorých možno dáta kombinovať 
a na základe ktorých je možné nový zoznam vygenerovať.

\subsubsection{Alexa}

Alexa, dcérska spoločnosť Amazon.com, publikovala na svojich stránkach od decembra 2008 až po začiatok augusta 2023 rebríček \textbf{Alexa top sites}, v ktorom zoraďovala 1 milión 
najnavšetevovanejších webstránok \cite{tranco-methodology}.
Zdrojom dát pre jeho tvorbu bolo rozšírenie pre webové prehliadače, ktoré si užívatelia mohli stiahnuť a po jeho inštalácií na vybranom prehliadači začalo zbierať a odosielať
dáta o prehliadaní internetu do Alexy. 
Pre účely vytvorenia hodnotenia stránok sa teda využíva sieťová prevádzka priamo vedená cez protokol HTTP(S). 
Rebríček Alexa sa v tomto skúmaní HTTP(S) prevádzky zaoberá doménami PLD (Pay Level Domains). 

Počet užívateľov s nainštalovaným rozšírením bol však obmedzený 
a to mohlo spôsovovať značné skreslenie celkových výsledkov kvôli malému zastúpeniu vzoriek prehliadania z celkovej sady -- všetkých používateľov internetu. 
Avšak, existujú zmienky, kde sa cituje pôvodná oficiálna stránka tohto zoznamu, kde autori tvrdili, že počet týchto užívateľov sa pohyboval v ráde niekoľkých miliónov \cite{tranco}.
Svoje hodnotenie Alexa zakladala na dvoch základných metrikách: \cite{kinsta-alexa-rank-article}\cite{tranco}
\begin{itemize}
    \item Počet návštevníkov stránky za daný deň (viac návštev od jedného sa počíta ako jediná návšteva)
    \item Spriemerovaný počet otvorení hocijakej podstránky (URL v rámci sledovanej domény)
\end{itemize}

Zo spomenutých metrík má vyššiu váhu pri rozhodovaní o popularite práve počet návštevníkov stránky za daný deň, čož môžeme podľa jej popisu nazvať aj počet unikátnych návštev za deň \cite{tranco}.
Obe metriky sa pre špecifický deň následne spriemerovali so všetkými takto napočítanými hodnotami za posledné tri mesiace, aby zoznam nadobudol určitú mieru stability. 
Tým pádom sú do úvahy pre každodenný zoznam vzané aj aktuálne trendy v prehliadaní. Vo výsledku sa tým predchádza nepresnému celkovému hodnoteniu popularity v prípadoch, 
kedy by neobvyklý jednorazový nárast v popularite doposiaľ neznámej stránky mal predbehnúť zaradenie inej, dlhodobo populárnej a vysoko zaradenej stránky v zozname. 
Koncom roka 2016 bol na nejakú dobu odstavený z prevádzky pre verejnosť na hlavných stránkach Alexy a znova bol sprístupnený pod záštitou Amazon Web Services ako platená služba \textbf{Alexa Top Sites}. 
Neskôr, začiatkom roka 2018 sa zhoršila jeho celková stabilita z toho dôvodu, že jeho algoritmus vyhodnocovania zaradenia stránok prestal brať ohľad na priemerovanie dát za predošlé mesiace.
Od vtedy, konkrétne 30.01.2018, bol zhotovovaný striktne iba za každý konkrétny deň \cite{tranco} až dokým táto služba nebola nadobro prerušená 01.08.2023, čo z nej teraz robí \textbf{zastaraný zoznam} \cite{tranco-methodology}.

\subsubsection{Majestic Million}

Majestic Million je služba, ktorá rovnako ako Alexa poskytuje zoznam webových domien zoradených podľa množstva spätných odkazov na ne ukazujúcich \cite{majestic-million-homepage}\cite{majestic-million-ranking}.
Ako individuálne technologické riešenie bolo uverejnené na webe Majestic 1.10.2012 \cite{majestic-million-publication}.
V jednoduchosti to znamená, že čím viac priamych odkazov na danú doménu sa na webe nájde, tým vyššie bude v tomto zozname zaradená. 
Majestic Million je vyvinutý spoločnosťou Majestic, ktorá sa zaoberá SEO (Search Engine Optimization) a skúmaním práve takýchto prepojení medzi doménami.
Majestic na dennej báze prehľadáva globálne zhruba 450 miliárd URL v rámci verejného internetu aby svoj zoznam zostavil. 
Hodnotenie je založené na počte spätných odkazov smerujúcich na jednotlivé domény. Tieto spätné odkazy, iným názvom \textbf{backlinks}, sú vlastne podsiete triedy C (IPv4 /24), ktoré odkazujú
na hodnotenú doménu minimálne jedenkrát.
Okrem každodenného vyhodnocovania aktuálneho zaradenia domén sa taktiež berie ohľad aj na priemer ich hodnotenia za posledných 120 dní \cite{tranco-methodology}.
Okrem iného je tiež vhodné spomenúť, že autori na svojom webe označujú vysoké zaradenie domény v tomto zozname za znak dôveryhodnosti \cite{majestic-million-homepage}.

V rámci celkového výstupného zoznamu Majestic Million sa berú do úvahy domény, \textbf{ale aj ich sub-domény}, takže sa môže stať, že napríklad doména \code{google.com} sa v ňom bude vyskytovať viackrát v podobe 
svojich sub-domén (\code{play.google.com}, \code{maps.google.com}, \code{mail.google.com} a iné) \cite{majestic-million-sub-domain-filtered}.

Na oficiálnom webe Majestic sú taktiež dostupné doplnkové služby, ktoré svojim užívateľom poskytujú naväzujúce užitočné informácie o týchto doménach. 
Jednou z nich je napríklad možnosť bližšie porovnať až do 10 domén, kde vstupom sú ich mená a výstupom je prehľadná tabuľka (takzvaná Buzz Table) zobrazujúca ich zaradenie a iné detaily, medzi ktoré patrí aj počet domén
referujúcich na tie vstupné a aj celkový počet ich externých backlinkov \cite{majestic-million-homepage}.
Existujú rôzne ďalšie služby, ktoré Majestic ponúka, no až na samotný rebríček a spomenutý Buzz Table sú tie ostatné spoplatnené, kde najlacnejšia možnosť platobného plánu začína na USD 46.99 mesačne \cite{majestic-million-pricing}.

\subsubsection{Cisco Umbrella}

Veľa domien nedosiahnuteľných (Unreachable)
\\
DNS
\\
Subdomény

\subsubsection{Quantcast}

Deprecated

\subsubsection{Chrome User Experience Report}

Dôležité vysvetliť kvôli kontextu v HTTP Archive

\subsubsection{Cloudflare Radar}

Najnovší perspektívny prídavok


\section{Web Crawling}
\label{web-crawling}

\todo{Obyčajná teória a vhodnosť Selenia ako výberu technológie pre túto prácu}

\subsection{Selenium}
\label{selenium}

\todo{Teória}

\section{HTTP Archive}
\label{httparchive}

HTTP Archive projekt sa zaoberá zaznamenávaním spôsobu konštrukcie a poskytovania digitálneho obsahu na webe. Je permanentným repozitárom informácií o webe a udržiava záznamy ako veľkosti
stránok, zlyhané HTTP požiadavky alebo technológie využité v rámci konkrétnej stránky. Vďaka týmto dátam je možné pozorovať trendy v histórií vývoja webu ako celku a zároveň je nad nimi môžné vykonávať
rôzne podrobné prieskumy a analýzy \cite{httparchive-about}. 

Autormi HTTP Archive sú rôzni individuáli z komunity zvanej Web Performance Group. Pôvodným autorom je Steve Souders, ktorý projekt založil v roku 2010 \cite{httparchive-faq}.
Momentálne sa na jeho údržbe po stránke vývoja podieľa švorica hlavných členov, a keďže ide o open source projekt, v prevádzke ho udržiavajú sponzori ako aj spoločnosti Google, Mozilla, O'Reilly Media a Fastly \cite{httparchive-about}. Záujemci ho môžu podporiť na stránkach Open Collective\footnote{\href{https://opencollective.com/httparchive}{https://opencollective.com/httparchive}}.
Taktiež je tento projekt súčasťou projektu Internet Archive, ktorý už od roku 1996 slúži ako digitálna knižnica poskytujúca zadarmo prístup ku knihám, filmom, hudbe a rovnako aj k miliardám archivovaných webstránok\footnote{\href{https://archive.org/}{https://archive.org/}}.

Cieľom projektu je vytvoriť a udržovať služby poskytujúce možnosť nahliadnuť do minulosti webu, pozorovať jeho prechod do momentálneho stavu a vďaka získaným náhľadom a poznatkom dokázať
predpovedať potencionálne nové trendy blízkej budúcnosti. 
Pre tento účel vyvinuli sadu nástrojov pre zbieranie spomínaných dát z verejného internetu, efektívne ukladanie nadobudnutých dát a ich reprezentáciu na svojom webe.
Naviac v rámci uskladnenia dát používajú službu \textbf{Google Cloud Platform (GCP)}, vďaka ktorej je možné dáta priamo na stránkach GCP prehliadať.
Keďže sú tieto dáta v rámci GCP verejne prístupné ako databázové tabuľky, prostredie GCP \textbf{Big Query} zastrešuje aj potrebu pre prostredie na prehliadanie dát pomocou SQL príkazov 
a vykonávanie komplexných analýz nad dátami HTTP Archive.

Vhodnosť projektu pre túto prácu je založená na tom, že ide o komuntitný projekt, ktorého výsledky sú verejne a zadarmo dostupné. Keďže umožňuje prístup k historickým záznamom reálneho prenosu HTTP komunikácie na webe, ktoré siahajú až po rok 2010, prirodzene sa z neho stáva primárny zdroj pre výskumy a analýzy, akou je aj táto.

\subsection{Proces získavania dát}

Základom pre všetky činnosti HTTP Archive sú dáta o stave webu. Tie sú získané pravidelným spúšťaním procesu zvaného \textbf{Web Crawling}.
Web Crawling je technika skúmania webu, ktorá programovo vstúpi na zvolenú stránku a získava o nej informácie ako metadáta, jej obsah a iné dáta v oblasti záujmu \cite{httparchive-webcrawling}.
HTTP Archive pomocou web crawlingu získava dáta ohľadom celkového aplikačného prenosu, kde meranou dátovou jednotkou je žiadosť HTTP \textbf{request} a odpoveď HTTP \textbf{response}, ktorou web server zareaguje. 
Keďže môžu nastať odlišnosti v komunikácií vedenej z bežného počítača oproti takej, ktorá je vedená z mobilného zariadenia, HTTP Archive zaznamenáva výsledky aj z \textbf{desktop}, aj z \textbf{mobilného} prostredia.
Zo získaných dát potom svojimi algoritmami extrahuje všetky dôležité poznatky, medzi ktoré patria napríklad aj stránkou používané zdroje a použité webové aplikačné rozhrania (Web API\footnote{\href{https://developer.mozilla.org/en-US/docs/Web/API}{https://developer.mozilla.org/en-US/docs/Web/API}}) \cite{httparchive-homepage}.

Výber vstupov do tohto procesu predstavuje nájsť vhodnú sadu záznamov URL na skúmanie. 
HTTP Archive na to momentálne používa projekt \textbf{Chrome User Experience Report}, spomínaný už v kapitole \ref{tranco-source-rankings}.

\subsubsection{WebPageTest}

Získané URL adresy sú teda použité ako vstup do programu WebPageTest. WebPageTest (ďalej označovaný už iba ako WPT) je softvér na testovanie výkonnosti webových stránok vyrobený spoločnosťou Google. Predstavuje komplexné riešenie schopné testovania a merania procesu načítavania, vykresľovania a využitia siete pre vybrané web stránky. 
Je zverejnený priamo na stránkach jeho oficiálneho repozitára GitHub\footnote{\href{https://github.com/catchpoint/WebPageTest}{https://github.com/catchpoint/WebPageTest}} spolu s priloženou dokumentáciou, a to pod licenciou Open Source.
Medzi konkrétne skúmané metriky patria napríklad: \cite{webpagetest}
\begin{itemize}
    \item Time to First Byte (TTFB) --- čas do prvej časti odpovede od servera
    \item First Contentful Paint (FCP) --- čas do začiatku načítavania obrázkov a grafiky
    \item Largest Contentful Paint (LCP) --- čas do načítania najväčšej časti obsahu stránky 
    \item Cumulative Layout Shitf (CLS) --- posun a zmena rozpoloženia obsahu stránky počas jej načítavania
\end{itemize}

HTTP Archive na svoje účely používa vlastnú WPT inštanciu. 
Táto inštancia je priebežne synchronizovaná s najnovšou dostupnou verziou.
Vo svojich behoch využíva užitočnú funcionalitu WPT --- vlastné (prispôsobené) metriky.
Pridanie vlastných metrík do WPT predstavuje spúšťanie hocijakej funkcie spísanej v jazyku JavaScript na konci behu testovania stránky. 
Využívaním tohto dokáže HTTP Archive zbierať akékoľvek dodatočné metriky zo svojich testovacích stránok \cite{webpagetest}.
Je dôležité poznamenať, že stránky sú testované s čistou vyrovnávacou pamäťou cache. Taktiež sa na stránkach vyžadujúcich autentifikáciu nikdy neprihlasuje.
To môže spôsobovať odchyľku oproti reálnemu používaniu testovacích web stránok. Ďalšou limitáciou je fakt, že každá stránka je preskúmaná samostatne a neberie sa žiaden ohľad na jej podstránky.
WPT je spúšťaný vždy prvý deň v mesiaci a teda obsahuje dáta za \todo{kedy ? - asi zistim pri GCP}. 

Po úspešnom pretestovaní celej vstupnej sady stránok príde na rad ukladanie získaných dát.
Pre účely uskladňovania je využitý formát HTTP Archive súboru (prípona \code{.har}, ďalej označovaný už len ako HAR).
Formát HAR je prispôsobený na uskladňovanie dát spojenia nadviazanom vo webovom prehliadači. Samotné dáta sú serializované ako JSON - JavaScript Object Notation\footnote{\href{https://www.json.org/json-en.html}{https://www.json.org/json-en.html}}.
Bežným obsahom HAR súboru býva HTTP žiadosť, prislúchajúca odpoveď, metriky výkonnosti načítania stránky a iné \cite{httparchive-harfile}.

Úspešne serializované a vhodne formátované dáta sú po skončení behu WPT nahrané do existujúcich databázových tabuliek na Google Cloud, čím sú sprístupnené pre používanie \cite{httparchive-faq}. 

\subsection{Google Cloud Platform}
% GC (Platforma, Moznosti, Platba, HAR Tables)

% -- BQ (Syntax, Priklad)

% Detekcia kompresie. Detekcia chýb...

% One of the great things about the BigQuery platform is how well it handles massive datasets and queries that can be incredibly computationally intensive

% At the time of this writing, the free tier allows 10GB of storage and 1TB of data processing per month

% Google Cloud Locations, USA

Description
\\
\textbf{Types of GCP BQ HTTP Archive tables}
\\
Big Query and query environment at GCP
\\
Query examples

\begin{center}
\noindent\includegraphics[width=3cm]{example-image}    
\end{center}

% \\
% Some of the types of tables you'll find useful when getting started are described below. These table names all follow the format yyyy\textunderscore mm\textunderscore dd\textunderscore desktop and yyyy\textunderscore mm\textunderscore dd\textunderscore mobile.

\subsection{HTTP Archive reporty}

Okrem samotných dát a prostredia na ich prehľadávanie zostavuje HTTP Archive projekt aj prehľady stavu webu formou interaktívnych grafov reprezentujúcich konkrétnu metriku v oblasti záujmu HTTP Archive.

\subsubsection{Elementárne reporty metrík}
Medzi takéto prehľady patrí napríklad report o zmene priemernej celkovej veľkosti konkrétnej načítanej stránky v kilobytoch, alebo, taktiež relevantý je aj report zobrazujúci dosiahnuteľnosť HTTP Archive --
počet jedinečných URL analyzovaných týmto projektom.

\begin{center}
\noindent\includegraphics[width=3cm]{example-image}    
\end{center}

% \todo{figure with description (total urls, description of graph, mention link to the SQL)}

\subsubsection{Web Almanac}
Na takýchto a mnoho ďalších nízko úrovňových reportoch každoročne stavia aj komplexný report s názvom \textbf{Web Almanac}, ktorý tiež patrí pod túto iniciatívu.
Web Almanac spája elementárne dáta do hodnotných kontextualizovaných náhľadov, ktoré približujú jeho čitateľom stav webu na vysokej úrovni \cite{httparchive-methodology}.
Predošlý rok bol štvrtým v poradí, za ktorý bol report zhotovený, čo sa vyzobrazuje na inkrementálne navyšovanej relevantnosti metrík, ktoré zahŕňa. 
Všetky tieto skúmané metriky sú zároveň dostupné na GitHub stránkach projektu\footnote{\href{https://github.com/HTTPArchive/almanac.httparchive.org/tree/main/sql/2022}{https://github.com/HTTPArchive/almanac.httparchive.org/tree/main/sql/2022}}, kde každú jednu reprezentuje hotový SQL skript spustiteľný priamo v prostredí GCP Big Query.

% WebPageTest, Lighthouse, Wappalyzer.... ?

Celý obsah reportu je dostupný na jeho samostatnej oficiálnej stránke\footnote{\href{https://almanac.httparchive.org/en/2022/table-of-contents}{https://almanac.httparchive.org/en/2022/table-of-contents}}. 
Každý aspekt jeho prieskumu je zatriedený do svojej vlastnej kategórie, ktorá sa v rámci obsahu označuje ako samostatná kapitola.
Čitatelia tu môžu nájsť napríklad kapitolu zameranú špecificky na JavaScript, použitie WebAssembly na webe, ale aj pre túto prácu relevantnejšie oblasti ako HTTP a bezpečnosť.

Celkovo sa snahou viacej ako 100 kontribútorov podarilo takouto formou štruktúrovane zaznamenať stav webu textovo, ale aj pomocou detailných grafových vizualizácií.
Autori sa snažia zvýšiť rozsah projektu a tým aj počet sledovaných relevantných oblastí tak, že každý rok povzbudzujú nových potencionálnych kontribútorov do pripojenia sa k ich iniciatíve\footnote{\href{https://almanac.httparchive.org/en/2022/methodology\#looking-ahead}{https://almanac.httparchive.org/en/2022/methodology\#looking-ahead}}. 

Do budúcna má Web Almanac dobré rozhľady. V tohtoročnom reporte bude oproti tomu minuloročnému zahrnutých viac ako dvojnásobok vzoriek URL\footnote{\href{https://httparchive.org/reports/state-of-the-web\#numUrls}{https://httparchive.org/reports/state-of-the-web\#numUrls}}, ktoré HTTP Archive podrobí svojmu web crawlingu.
V ideálnom prípade to znamená, že sa dosah podkladov pre report efektívne zdvojnásobí a tým pádom má potenciál byť presnejší a globálne reprezentatívnejší.


\section{Alternatívy k HTTP Archive}
\label{httparchive-alternatives}

Aj keď je HTTP Archive najvhodnejším zdrojom dát pre účely tejto práce, existujú aj iné, ktoré stoja za zmienku.
Dáta, ku ktorým máme prístup vďaka tomuto primárnemu zdroju síce sú postačujúce pre hocijaký typ analýzy, no sú dostupné aj zdroje disponujúce špecifickými výhodami, ktoré
zase umožňujú či už spätne kontrolovať správnosť dát HTTP Archive, alebo na ne naviazať.
Z toho dôvodu sú niektoré alternatívy popísané v tejto kapitole.

\subsection{crawler.ninja}

Projekt \textbf{crawler.ninja}\footnote{\href{https://crawler.ninja/}{https://crawler.ninja/}} založený autorom Scottom Helme-om slúži ako podklad pre jeho prieskum
webu, v ktorom pozoruje stav bezpečnosti na internete. 
Výsledky tohto prieskumu autor prvýkrát zverejnil už v roku 2015, no o vytvorení projektu crawler.ninja na svojom blogu píše až v júli 2018 \cite{crawler-ninja}. 

Úmyslom autora pozorovať bezpečnosť vyúsťuje do jeho periodických reportov týkajúcich sa tejto tématiky. 
Samotný projekt sa stará o zber dát pre zhotovenie týchto reportov.
Aj keď zbieranie dát pretrváva do súčasnosti, posledný report od autora bol publikovaný dávnejšie --- 09.12.2021, od kedy zanechal svoju pravidelnosť (aspoň jeden report za rok).

Ninja.crawler slúži na prehľadávanie webu technikou Web Crawling za cieľom získať dáta o web stránkach, ktoré autor plánuje skúmať.
Ako vstup do tohto procesu, a teda zoznam použitých URL, sa používa rebríček populárnych stránok Alexa Top 1 milion.

Crawl sa spúšťa každý deň a získané dáta ukladá (podľa toho čo autor zmieňuje na svojom blogu) do databázy MySQL, z ktorej je následne vytvorený takzvaný databázový export.
Databázový export predstavuje súbor SQL príkazov, spustením ktorých môže hocikto replikovať pôvodnú databázu aj s obsahom jej tabuliek. 
Možnosť vytvoriť takýto súbor poskytuje priamo MySQL databáza. 
Jednou z možností ako ho vytvoriť je použitím pomocného programu na tvorbu databázových záloh --- \code{mysqldump} \cite{mysql-doc}.
Toto je dôležité pre toho, kto chce výsledné dáta použiť. 
Na oficiálnej stránke crawler.ninja autor tieto súbory periodicky (ale nie priebežne za každý deň) zverejňuje pod licenciou \textit{CC BY-SA 4.0}\footnote{\href{https://creativecommons.org/licenses/by-sa/4.0/}{https://creativecommons.org/licenses/by-sa/4.0/}}, takže sú veškeré získané dáta použiteľné pre študijné, ale aj komerčné účely potencionálnych záujemcov.
Sú dostupné vo forme priamo stiahnuteľných archívov ZIP pomenovaných vždy podľa dátumu, za ktorý boli nazbierané.
Ako je spomenuté vyššie, ten, kto chce dáta na svoje účely využiť si ich musí najskôr importovať do svojej databáze MySQL, kde s nimi môže začať pracovať.
Docieliť toho je možné napríklad shell príkazom \code{\textbackslash source} v administrátorskej konzole MySQL \cite{mysql-doc}.

Mimo uvedených dát v podobe databázových exportov sú od Scotta dostpné taktiež konkrétne, pre neho významné, postupne nazbierané metriky, ktoré v reportoch o svojich prieskumoch používa či už priamo, alebo v rôznych kombináciach pri tvorbe grafov. Taktiež sú zverejnené na jeho webe\footnote{\href{https://crawler.ninja/files/}{https://crawler.ninja/files/}}.

\subsubsection{Výhody a nevýhody}

\todo{napisanim tejto kapitoly zistim aj nejake nevyhody HTTP Archive - potom ich mozem pripisat do textu vyssie}
\todo{Porovnať pravidelnosť dát z http archive, manuálne sťahovanie, kompletne zadarmo, interval dostupnosti dát - každým novým uploadom sa stráca najdávnejšia história}

\subsection{Čo ešte ?}

Chat GPT found alternatives
