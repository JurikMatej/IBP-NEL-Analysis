\chapter{Zdroje dát pre analýzu}
\label{data-sources-available-for-research}

% # PART 2
% # TODO obsah by mal napovedat, ze vysledky boli dosiahnute
%        nejčastěji navštěvovaných stránek
%        Selenium
%        výhody a nevýhody vlastnych a externych dat
% # TODO sekcia o automatizovanom prehliadani stranok, Playwright
% # TODO nie je az tak dolezite, ze su najnavstevovanejsie, ako ze sa to automatizuje
% # TODO spomenut ale aj Selenium

Poznávacím znamením využívania technológie NEL je prítomnosť hlavičiek \code{NEL} a \code{Report-To} v HTTP odpovediach z monitorovaných web serverov. 
Pre účely analýzy jej využívania je teda nutné získať dáta obsahujúce tieto odpovede.
Cieľom je pozerať sa buď na reálne komunikácie, ktoré už prebehli, alebo skúmať ako web servery aktuálne dosiahnuteľné na internete
odpovedajú na HTTP požiadavky. 
V tejto kapitole sú detailne popísané dostupné zdroje dát a konkrétne spôsoby, akými možno záznamy takýchto komunikácií získať.

\section{Potrebné dáta}

V prvom rade je nutné definovať množinu webových serverov, na ktoré sa možno zamerať pri získavaní potrebných záznamov HTTP komunikácie.
Vzhľadom na to, že web servery možno adresovať pomocou ich doménového mena, ďalej tieto web servery označujem ako skúmané domény.
Množinu skúmaných domén možno zaobstarať z niekoľkých zdrojov, ktoré sú popísané v sekcií \ref{tranco}.

Po definovaní množiny skúmaných domén je možné zmienené záznamy o komunikácií \mbox{s nimi} získať podľa toho, kedy komunikácia prebehla:
\begin{enumerate}
    \item Súčasnosť -- aktuálne dáta:
    
    Z hľadiska súčasnosti je možné jednoducho prehliadať web a zbierať pri tom HTTP odpovede, ktoré použitý webový prehliadač zaznamená. 
    Pre veľké množstvo skúmaných domén je tento proces však nutné automatizovať.
    Na účel toho je možné použiť už existujúce technológie pre automatizáciu prehliadania webu ako Selenium alebo Playwright, ktoré opisuje sekcia \ref{selenium}.

    \item Minulosť -- historické dáta:

    Pre sledovanie historického vývoja nasadenia NEL je tiež nevyhnutné nahliadnuť do histórie prevádzky skúmaných domén. 
    Z tohto hľadiska je nutné použiť už spracované a uložené dáta. 
    Vyhovujúcim zdrojom historických dát je projekt \textbf{HTTP Archive}, ktorému sa táto práca venuje primárne, a to v sekcií \ref{httparchive}.
    Ide o projekt, v rámci ktorého bol zaznamenávaný vývoj webu od roku 2010. Vhodnosť projektu potvrdzuje skutočnosť, že prvá stabilná špecifikácia, podľa ktorej je NEL skúmaný v tejto práci, bola publikovaná až v roku 2018. HTTP Archive teda pokrýva celý vývoj NEL v čase.
\end{enumerate}

\pagebreak

\section{Množina skúmaných domén}
\label{tranco}

% Platí, že čím je mohutnejšia množina skúmaných domén, tým presnejšiu analýzu využívania NEL na webe možno vypracovať.
% Preto je pri výbere takejto množiny kľúčová jej mohutnosť.
Existujú projekty, ktoré sa zaoberajú získavaním doménových mien pre rôzne účely.
Napríklad, niektoré projekty sú vypracované s cieľom spravovať zoznam domén, ktoré často navštevujú používatelia služieb patriacich autorom daného projektu \cite{chrome-crux}. 
Z tých je vzhľadom na zadanie tejto práce relevantný projekt Chrome User Experience Report, ktorý je používaný v rámci projektu HTTP Archive. 
Popísaný je v nasledujúcej sekcií.

Iné projekty zase uchovávajú tie domény, ktorých webstránky sú jednoducho najčastejšie navšetevované \cite{hacker-target-website-lists-overview, tranco}.
Takým je napríklad projekt TRANCO, ktorý popisujem v sekcií \ref{tranco}.

\subsection{Domény navštevované používateľmi prehliadača Google Chrome}
\label{chrome-user-experience-report}

Chrome User Experience Report, skrátene CrUX, je dataset uschovávajúci dáta získané používateľmi prehliadača Google Chrome pri prehliadaní populárnych domén na webe \cite{chrome-crux}.
Obsiahnuté dáta predstavujú sledované metriky týkajúce sa rôznych aspektov prehliadaných webstránok.
Podľa dokumentácie sa tieto metriky zbierajú výhradne z webstránok, pre ktorých domény platí, že sú dosiahnuteľné a dostatočne populárne.
Zoznam týchto domén je možné získať z priamo z CrUX datasetu.
Je totiž skladovaný v prostredí služby Google Cloud BigQuery, kde je verejne prehliadateľný. 
Dateset je aktualizovaný novými dátami každý mesiac.
Službu BigQuery bližšie popisujem v sekcií \ref{big-query}.

Nenašiel som presné podrobnosti o tom, akým spôsobom sú domény získavané, no ako populárne sú označované podľa počtu ich návštevníkov.
Aby bola určitá doména zaradená do zoznamu domén, s ktorými CrUX pracuje, musí počet jej návštevníkov presahovať predom definované číslo.
Toto číslo sa však autori dokumentácie rozhodli neuverejniť \cite{chrome-crux-methodology}.

Podľa samotného datasetu v prostredí BigQuery je však možné presvedčiť sa, že CrUX spravuje veľké množstvo týchto populárnych domén.
Napríklad, pre september 2018, dátum, kedy bola publikovaná prvá stabilná špecifikácia NEL (viď sekciu \ref{network-error-logging}), je počet spravovaných jedinečných domén CrUX 4 375 805.
Pre aktuálnejší dátum, marec 2024, je to 18 669 191.


\subsection{Najčastejšie navštevované domény}

Jeden z projektov, ktorých účelom je jednoducho zaznamenávanie domén s najčastejšie navštevovanými webstránkami, je TRANCO \cite{tranco}.
Projekt TRANCO poskytuje sadu nástrojov pre generovanie rebríčkov najpopulárnejších domén. 
Tieto rebríčky webových domén zoradených podľa hodnotenia návštevnosti ich webstránok je odolný proti externej manipulácií a vhodný na účely výskumu. 
Vznikol na základe častej potreby pre skúmanie práve takýchto domén či už pre jednoduchú referenciu, alebo ako podklad pre ďalší prieskum.
TRANCO nie je prvým takýmto rebríčkom, ale je prvým, ktorý sa zaoberá nedostatkami iných existujúcich rebríčkov.
Existujúce rebríčky využíva ako zdroje dát pre tvorbu toho svojho.
Nový rebríček TRANCO sa tvorí zlučovaním zdrojových rebríčkov do nového, stabilnejšieho rebríčka, ktorý reprezentuje domény v globálnej škále.
TRANCO dokonca do určitej miery odstraňuje zo zdrojových rebríčkov domény, na ktorých môže byť zverejnený nežiadúci (nebezpečný) obsah.
Obsahuje primárne registrovateľné domény, čo sú domény, ktoré si môže priamo zakúpiť či už jednotlivec alebo organizácia. 
Príkladom môžu byť domény registrované pod TLD \code{com}, ale aj domény registrované pod eTLD zo zoznamu verejných suffixov ako \mbox{\code{co.uk} \cite{tranco}}.

\pagebreak

TRANCO je podložený štúdiom zameraným na vylepšenie dostupných alternatív rebríčkov populárnych domén.
Práve tieto alternatívne rebríčky tvoria vnútorne použité zdroje dát pre výsledné rebríčky TRANCO.
Medzi ne patria projekty \cite{tranco-config}:
\begin{itemize}
    \item Alexa Top Sites,
    \item Majestic Million,
    \item Cisco Umbrella,
    \item Quantcast,
    \item Chrome User Experience Report,
    \item Cloudflare Radar.
\end{itemize}
TRANCO je ako zdroj skúmaných domén vhodný, pretože sprístupňuje a vylepšuje presnosť záznamov z týchto alternatív.
Taktiež udržiava ich historické dáta, ktoré už nie sú samostatne dostupné.
Vzhľadom na to je možné preskočiť výber toho najvhodnejšieho \mbox{z pomedzi} nich a použiť práve TRANCO.

\subsubsection{Generovanie rebríčka TRANCO}
\label{tranco-generation}

V jeho štandardnej forme, rebríček TRANCO sa generuje každý deň v dvoch verziách:
\begin{itemize}
    \item TRANCO rebríček domén
    \item TRANCO rebríček domén a subdomén
\end{itemize}

V tomto každodenne generovanom rebríčku sú prednastavené aj zdroje dát, teda použité zdrojové rebríčky, aj dátumový rozsah, za ktorý sa zo zdrojových rebríčkov má čerpať.
Štandardne sa vytvára výsledok z rebríčkov Chrome User Experience Report, Majestic Million, Cloudflare Radar a Cisco Umbrella.
Dátumový rozsah je nastavený na posledných 30 dní \cite{tranco-config}, pričom TRANCO použije ako kompletný zdroj dát všetky spomenuté rebríčky vygenerované za túto dobu. 

V novom TRANCO rebríčku sa zo zdrojových rebríčkov spriemeruje pre každú doménu jej zaradenie aplikovaním jednej z dostupných kombinačných metód pre upravenie finálneho hodnotenia domén.
Pre štandardný rebríček je využitá kombinačná metóda takzvanej harmonickej progresie, nazývaná Dowdall rule. 
Dowdall rule hodnotí v zozname obsahujúcom N domén prvú hodnotou 1 a všetky ostatné postupne \(1/2\), \(1/3\) až \(1/(N-1)\) a zakončí hodnotením poslednej -- \(1/N\) \cite{tranco, tranco-config}.
Názornú ukážku aplikovania Dowdall rule na rebríčky vizualizuje obrázok \ref{img:dowdall-rule}.
Generuje sa v ňom nový rebríček o veľkosti štyroch domén, takže sa do úvahy berú prvé štyri domény z každého vstupného rebríčka. 
Skóre sa vypočíta podľa umiestnenia a podľa celkového počtu domén radených v danom zdrojovom rebríčku. 
Rebríček 4 (v obrázku úplne napravo) s najvyšším počtom hodnotených domén výsledok ovplyvnil najviac.

\begin{figure}[htb]
\begin{center}
 % \includegraphics[scale=0.375]{obrazky-figures/dowdall_rule.png}
 \includegraphics[scale=0.84]{obrazky-figures/dowdall_rule_size_fit_cropped.pdf}
 \caption{Príklad uplatnenia Dowdall rule na spriemerovanie štyroch vstupných rebríčkov do výsledného rebríčka TRANCO.}
 \label{img:dowdall-rule}
\end{center}
\end{figure}

\pagebreak

Po dokončení priemerovania a zoraďovania sa spolu s výsledným rebríčkom vytvorí na oficiálnom webe TRANCO aj jedinečná stránka obsahujúca odkaz na jeho stiahnutie a tiež citácia, 
ktorou je možné jedinečne odkazovať na tento nový rebríček v prácach, ktoré ho môžu použiť na svoje účely.
Zároveň, okrem generovania nových TRANCO rebríčkov sú na oficiálnej stránke dostupné aj historicky vygenerované, spomenuté štandardné, každodenné rebríčky \cite{tranco-homepage}.

Príklad obsahu vygenerovaného rebríčka znázorňuje výpis \ref{listing:tranco-obsah}.
Rebríček je možné stiahnuť vo formáte \code{zip} archívu, ktorý obsahuje práve jeden súbor vo formáte \code{csv}. 
Obsahom je na každom riadku čiarkou oddelené umiestnenie domény a jej meno.

\begin{center}
\centering
\begin{lstlisting}[
caption={Ukážka vygenerovaného denného rebríčka TRANCO z 12. januára 2024, orezaného na prvých 10 domén.},
label=listing:tranco-obsah, 
language=json, 
frame=lb,
xleftmargin=.3875\textwidth, 
xrightmargin=.3875\textwidth]
1,google.com
2,amazonaws.com
3,facebook.com
4,microsoft.com
5,a-msedge.net
6,googleapis.com
7,apple.com
8,youtube.com
9,akamaiedge.net
10,akamai.net

\end{lstlisting}
\end{center}


\section{Automatizované prehliadanie webu}
\label{selenium}

Pre automatizáciu prehliadania webu bolo definované aplikačné programové rozhranie nazvané WebDriver \cite{crawling-webdriver}, ktorého implementácie umožňujú programovo ovládať webový prehliadač.
WebDriver má aj rôzne iné prípady použitia, no v rámci tejto práce je dôležitá funkcionalita automatizovať webový prehliadač pri načítavaní webstránok a zdrojov nimi používaných. 

\subsection{Získavanie dát}

Pomocou špecifických príkazov je možné programovo spustiť inštanciu ľubovoľného webového prehliadača, navštíviť zvolenú doménu pomocou adresy URL a po úspešnom načítaní získanej webstránky pracovať s jej obsahom.
Pre podrobné preskúmanie nasadenia NEL na danej doméne je žiadúce na nej prehľadať viaceré webstránky.
Preto je ďalším krokom jej automatizovaného prehliadania extrahovať z obsahu získanej webstránky dostupné hyperlinky odkazujúce na ďalšie webstránky tej istej domény a prehľadať aj tie.
V rámci tohto procesu sa dáta pre analýzu nasadenia NEL získavajú ukladaním záznamov hlavičiek HTTP odpovedí pre načítané webstránky a nimi používaných zdrojov.

\subsection{Selenium WebDriver}

Príkladom populárnej implementácie takéhoto rozhrania je Selenium WebDriver \cite{crawling-selenium}.
Selenium je dostupný pre viaceré programovacie jazyky.
Konkrétnou limitáciou Selenium je však skutočnosť, že neimplementuje funkcionalitu pre čítanie hlavičiek HTTP odpovedí získaných zdrojov.
Je ale stále možné funkcionalitu doplniť knižnicami tretej strany alebo použitím alternatívnych metód pre jej dosiahnutie, ktoré tu však nerozoberám.

\subsection{Playwright}

Alternatívou pre Selenium WebDriver je napríklad Playwright.
Rovnako implementuje rozhranie pre ovládanie webových prehliadačov,
pričom tiež podporuje viaceré programovacie jazyky.
Funkcionalita pre čítanie HTTP komunikácie je ale v tomto prípade zabudovaná priamo do vlastného rozhrania Playwright \cite{crawling-playwright}.
Pre jednoduchosť v implementácií zaznamenávania potrebných HTTP hlavičiek je Playwright vhodnejším výberom pre nástroj automatizujúci prehliadanie webu.

\subsection{Výhody a nevýhody}

Jednoznačnou prednosťou tejto metódy získavania vlastných dát pre analýzu je skutočnosť, že iným spôsobom nie je možné získať rozsiahle objemy dát popisujúce \textit{aktuálny} stav nasadenia NEL.
Tiež je výhodou, že nie je nutné platiť za získavanie dát takýmto spôsobom, nerátajúc náklady na prevádzku počítača, na ktorom sa má automatizované prehliadanie webu spúšťať.

Značnou nevýhodou však je, že v prípade skúmania množstva domén a ich webstránok tento proces trvá veľmi dlho, a to kvôli nutnosti čakať na načítanie každej skúmanej webstránky. 
Okrem toho zároveň zo zamerania tejto metódy na súčasnosť vyplýva, že nie je možné získať historické dáta pred prvým spustením hotového nástroja na automatizované prehliadanie webu.  

% Web Crawling je technika skúmania webu, ktorá programovo vstúpi na zvolenú stránku a získava o nej informácie ako metadáta, jej obsah a iné dáta v oblasti záujmu \cite{httparchive-webcrawling}.

\section{HTTP Archive}
\label{httparchive}

Projekt HTTP Archive sa zaoberá zaznamenávaním spôsobu konštrukcie a poskytovania digitálneho obsahu na webe. 
Je repozitárom informácií o webe a udržiava záznamy ako veľkosti
stránok, zlyhané HTTP požiadavky alebo technológie využité v rámci konkrétnej stránky. 
Vďaka týmto dátam je možné pozorovať trendy v histórií vývoja webu ako celku a zároveň je nad nimi možné vykonávať rôzne podrobné prieskumy a analýzy \cite{httparchive-about}. 

Autormi HTTP Archive sú členovia komunity zvanej Web Performance Group. Pôvodným autorom je Steve Souders, ktorý projekt založil v roku 2010 \cite{httparchive-faq}.
Momentálne sa na jeho údržbe po stránke vývoja podieľa štvorica hlavných členov, a keďže ide o open source projekt, v prevádzke ho udržiavajú sponzori ako aj spoločnosti Google, Mozilla, O'Reilly Media a Fastly.
Taktiež je tento projekt súčasťou projektu Internet Archive, ktorý už od roku 1996 slúži ako digitálna knižnica poskytujúca prístup ku knihám, filmom, hudbe \mbox{a rovnako} aj \mbox{k miliardám} archivovaných webstránok \cite{httparchive-about}.

Cieľom projektu je vytvoriť a udržiavať služby poskytujúce možnosť nahliadnuť do histórie webu, pozorovať jeho prechod do momentálneho stavu a vďaka získaným poznatkom dokázať
predpovedať potencionálne nové trendy blízkej budúcnosti. 
Pre tento účel vyvinuli sadu nástrojov pre zbieranie uvedených dát z webu, efektívne ukladanie nadobudnutých dát a ich reprezentáciu na svojej webovej stránke.
Na uskladnenie dát sa používa služba \mbox{\textbf{BigQuery} poskytovaná} na platforme \textbf{Google Cloud Platform} (ďalej už iba GCP).
Tieto dáta sú verejne prístupné ako databázové tabuľky v prostredí BigQuery.
Prehliadať ich možno pomocou príkazov jazyka Structured Query Language, zaužívaného štruktúrovaného jazyka pre správu dát uložených v databáze (ďalej ako SQL).

Vhodnosť projektu pre túto prácu spočíva v tom, že ide o komunitný projekt, ktorého výsledky sú verejne dostupné. 
Keďže umožňuje prístup k historickým záznamom reálneho prenosu HTTP komunikácie na webe, ktoré siahajú až po rok 2010, prirodzene sa z neho stáva primárny zdroj pre výskumy a analýzy, akou je aj analýza v rámci tejto práce.


\subsection{Získavanie dát}
\label{fetching-data}

Ako bolo zmienené, HTTP Archive získava dáta z webu. 
Získava ich pomocou automatizovaného prehliadania webu (viď sekciu \ref{selenium}).
Cieľové dáta predstavujú celkový aplikačný prenos na prehliadaných doménach, kde meranou dátovou jednotkou je HTTP žiadosť \mbox{a HTTP} odpoveď, ktorou na žiadosť daná doména zareaguje.
HTTP Archive zaznamenáva výsledky separátne získané z prostredia aj počítačového (desktop), aj mobilného zariadenia.
Zo získaných dát potom svojimi algoritmami extrahuje všetky dôležité poznatky, medzi ktoré patria napríklad aj stránkou používané zdroje a použité webové aplikačné rozhrania (Web API) \cite{httparchive-homepage}.

Ako zoznam vstupných domén do tohto procesu HTTP Archive momentálne používa projekt Chrome User Experience Report \cite{httparchive-faq}, popísaný v sekcii \ref{chrome-user-experience-report}.

\subsubsection{WebPageTest}

Nástroj použitý pre automatizované prehliadanie webu je WebPageTest. Tento nástroj (ďalej označovaný už iba ako WPT) je softvér na testovanie výkonnosti webstránok vyvinutý spoločnosťou Google \cite{webpagetest}. 
Predstavuje komplexné riešenie schopné merať rôzne metriky ako proces načítavania, vykresľovania a využitia siete pre vybrané web stránky. 
Je zverejnený priamo na stránkach jeho oficiálneho repozitára GitHub\footnote{\url{https://github.com/catchpoint/WebPageTest}} spolu s priloženou dokumentáciou, a to pod open source licenciou.

HTTP Archive na svoje účely používa vlastnú WPT inštanciu. 
Táto inštancia je priebežne synchronizovaná s najnovšou dostupnou verziou.
Vo svojich behoch využíva doplnkovú funkcionalitu WPT --- vlastné (prispôsobené) metriky.
Pridanie vlastných metrík do WPT predstavuje spúšťanie ľubovolnej funkcie napísanej v jazyku JavaScript na konci behu testovania danej webstránky. 
Použitím tejto funkcionality HTTP Archive dokáže zbierať akékoľvek dodatočne vypočítané metriky z webstránok domén, pre ktoré zbiera \mbox{dáta \cite{webpagetest}}.

Je dôležité poznamenať, že stránky sú testované s čistou vyrovnávacou pamäťou cache. 
Taktiež sa na stránkach vyžadujúcich autentifikáciu nikdy neprihlasuje.
To môže spôsobovať odchýlku oproti reálnemu scenáru používania navštevovaných web stránok. 
WPT je spúšťaný vždy prvý deň v mesiaci, takže HTTP Archive zverejňuje dáta na mesačnej báze.

Pre účely uskladňovania získaných dát je využitý formát HTTP Archive súboru (prípona \code{.har}, ďalej označovaný už len ako HAR).
Formát HAR je prispôsobený na uskladňovanie dát spojenia nadviazanom vo webovom prehliadači. Samotné dáta sú serializované vo formáte JSON.
Bežným obsahom HAR súboru býva HTTP žiadosť, prislúchajúca odpoveď, metriky výkonnosti načítania stránky a iné \cite{httparchive-harfile}. 
Orientačný príklad obsahu takéhoto súboru je vo výpise \ref{listing:harfile}.
Všetky detaily týkajúce sa procesu získania webstránky sú zaznamenané v položke \code{log}. Pole \code{version} definuje verziu súboru HAR. Pole \code{pages} obsahuje napríklad URL získaného zdroja a pole \code{entries} obsahuje HTTP žiadosti \mbox{a odpovede pre daný zdroj a zdroje ním používané.}

\begin{center}
\centering
\begin{lstlisting}[
caption={Orientačná ukážka obsahu súboru HAR.},
label=listing:harfile, 
language=json, 
frame=tb,
xleftmargin=.09\textwidth, 
xrightmargin=.09\textwidth]
{
  "log": {
    "version": "1.2",
    ...
    "pages": [
      {
        "startedDateTime": "2024-01-12T15:25:01.278Z",
        "id": "page_1",
        "title": "https://tranco-list.eu/list/KJ49W/1000000",
        "pageTimings": {...}
      }
    ],
    "entries": [
      {
        ...
        "request": {...},
        "response": {...},
        ...
      },
      ...
    ]
  }
}
\end{lstlisting}
\end{center} 

\subsection{Ukladanie dát}
\label{httparchive-data-storage}

Dáta získané pomocou WPT sa nahrávajú do existujúcich databázových tabuliek prostredia BigQuery, čím sú sprístupnené pre prehliadanie \cite{httparchive-faq}.

\subsubsection{BigQuery}
\label{big-query}

BigQuery, infraštruktúra pre ukladanie dát v rámci GCP, je produkt, ktorý umožňuje jeho užívateľom spravovať a analyzovať dáta za pomoci vstavaných funkcionalít ako napríklad aj strojového učenia \cite{google-bq}. Dôležitou vlastnosťou Big Query je prispôsobenosť na vysokorýchlostné výpočty nad obrovským množstvom dát.
Distribúcia výpočtov umožňuje docieliť vykonávanie analýzy nad dátami o veľkosti v terabajtoch za sekundy (TB/s) a petabajtoch za minúty (PB/m).
K tomu napomáha špeciálna vnútorná reprezentácia uložených tabuliek. 
Bežný spôsob ukladania dát do tabuliek v databáze je takzvaný riadkovo orientovaný.
Orientovanie na riadky znamená, že sa záznamy v tabuľke ukladajú priamo vedľa seba na disk databázy.
To je vhodné pre prípady, keď sú na úložisku záznamy vyhľadávané individuálne.
Avšak, pre zložité analytické výpočty nad veľkým objemom dát to predstavuje problém vo výkonnosti, pretože sa musia potupne pre každý záznam tabuľky prehľadať všetky jeho polia \mbox{(stĺpce) \cite{google-bq-storage}}.

\begin{figure}[htb]
\begin{center}
 \includegraphics[scale=0.7]{obrazky-figures/row-oriented-store.png}
 \caption{Vizualizácia uskladnenia dát v riadkovo orientovanej databáze. Obrázok prevzatý z \cite{google-bq-storage}.}
 \label{img:row-oriented-store}
\end{center}
\end{figure}

Riešenie, ktorým BigQuery tento problém adresuje je použitie orientácie na jednotlivé stĺpce. 
Ukladaním dát v stĺpcovom formáte, a teda ukladaním každého stĺpca separátne umožňuje prehľadávať dataset bez viazania sa na všetky ostatné stĺpce.
Tým sa efektívne znižuje množstvo dát, ktoré sa prehľadávajú naraz.
Takto je databáza optimalizovaná pre analýzy nad obrovským množstvom uložených záznamov \cite{google-bq-storage}.

\begin{figure}[htb]
\begin{center}
 \includegraphics[scale=0.7]{obrazky-figures/column-oriented-store.png}
 \caption{Vizualizácia uskladnenia dát s využitím orientácie na jednotlivé stĺpce. Obrázok prevzatý z \cite{google-bq-storage}.}
 \label{img:column-oriented-store}
\end{center}
\end{figure}

\pagebreak

Dáta skladované v BigQuery sú organizované do skupín klasických databázových tabuliek nazývaných dataset.
Verejne je dostupné množstvo datasetov pre prehľadávanie.
Najprv je ale nutné založiť si Google Cloud projekt na oficiálnej stránke, ktorý slúži ako priestor mien pre zdroje, ktoré užívateľ do projektu pridáva a používa.
K dátam je možné priamo pristupovať prostredníctvom troch rozhraní \cite{google-bq}:
\begin{enumerate}
    \item Google Cloud Console:

    Webové grafické rozhranie pre spravovanie Google Cloud projektov. 
    Časť Google Cloud Console, ktorú užívatelia môžu využiť špecificky na prehliadanie dát BigQuery sa nazýva \textbf{BigQuery Studio}.
    Výhodou tohto spôsobu pracovania so zdrojmi je vysoká úroveň interaktivity, ktorú ponúka zabudované integrované vývojové prostredie pre prácu s dátami. 

    \item BigQuery nástroj príkazového riadku:

    Pre zobrazovanie databáz a tabuliek, prehľadávanie a spravovanie dát v prostredí príkazového riadka je možné využiť nástroj s názvom \code{bq}. 
    
    \item BigQuery klientske knižnice

    Vďaka klientským knižniciam implementujúcim komunikačné rozhranie s BigQuery je taktiež dostupná možnosť programovo manipulovať a prehliadať zdroje priradené ku Google Cloud projektu používateľa.
    Táto možnosť je vhodná pre predom definované, opakované úlohy, ktoré či už požadujú zdroje na vstupe, alebo ich počas svojho behu nahrávajú, prípadne upravujú podľa potreby.
\end{enumerate}

Pri využití ktorejkoľvek z týchto možností platí, že prehliadanie a manipuláciu dát umožňuje jazyk SQL \cite{google-bq}.
V prostredí BigQuery sa používa dialekt pre SQL nazývaný \textbf{GoogleSQL}\footnote{\url{https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax}}.
Ako už bolo uvedené, HTTP Archive ukladá svoje výstupné dáta práve do BigQuery.
Tieto dáta možno nájsť pomocou Google Cloud Console dostupné ako \textit{zdroj} (anglicky resource), ktorý si môže prihlásený používateľ pridať do svojho projektu.

\pagebreak

\noindent Po pridaní tohto zdroja s priradeným názvom \code{httparchive} do projektu\footnote{\url{https://github.com/HTTPArchive/httparchive.org/blob/main/docs/gettingstarted\_bigquery.md}} sa sprístupnia pre používanie datasety ako napríklad \cite{httparchive-repo}:

\begin{itemize}
    \item \code{summary\_pages}:

    Obsahuje detaily o jednotlivých web stránkach ako časy ich načítania, počet žiadostí o jej zdroje, typy zdrojov a ich veľkosti.
    Taktiež sú tu informácie týkajúce sa presmerovaní, vzniknutých chýb, použitých služieb ako CDN\footnote{\url{https://www.cloudflare.com/learning/cdn/what-is-a-cdn/}} a iné.
    
    \item \code{summary\_requests}:

    Nachádzajú sa tu dáta o konkrétnych objektoch načítaných ako už spomínané zdroje pre webstránky v datasete \code{summary\_pages}.
    V dátach je možné prehľadávať ako boli zdroje načítané priamo v hlavičkách HTTP odpovede, v ktorej prišli zo serveru poskytujúceho danú webstránku.
    
    \item \code{pages}:

    Extrahované HAR súbory pre každú URL z prehľadávaných webstránok.
    
    \item \code{requests}:

    Extrahované HAR súbory pre každý zdroj jednotlivých prehľadávaných webstránok \mbox{v \code{pages} datasete}.
    
    \item \code{response\_bodies}:

    Extrahované HAR súbory obsahujúce celé telo HTTP odpovede z každej URL prehľadávaných web stránok.
    Ide o veľmi veľké tabuľky, ktoré môžu dosahovať veľkosť \mbox{v jednotkách terabajtov (TB)}.
\end{itemize}

BigQuery zdroj \code{httparchive} sprístupňuje aj niekoľko ďalších datasetov. Táto práca sa jednoznačne najviac zaoberá datasetom \code{summary\_requests}.

Každý z týchto datasetov obsahuje tabuľky nazvané podľa rovnakej konvencie --- dátum vykonaného zberu dát a prostredie, v akom prebiehal.
Dátum je definovaný formátom \code{YYYY\_MM\_DD}, kde \code{YYYY} predstavuje rok, \code{MM} mesiac a \code{DD} deň. 
Prostredie môže byť buď počítačové alebo mobilné, ako sa spomína už v sekcii \ref{fetching-data}.
Príkladom názvu tabuľky teda môže byť \code{2018\_01\_15\_mobile} alebo \code{2023\_01\_01\_desktop}.
Za použitia GoogleSQL je možné tabuľky kombinovať a vytvárať komplexné sady dát pre ďalšiu analýzu.

\begin{figure}[htb]
\begin{center}
 \includegraphics[scale=0.53]{obrazky-figures/bigquery_response_bodies.png}    
 \caption{Pohľad na časť otvoreného okna s tabuľkou \code{2023\_01\_01\_desktop} v prostredí BigQuery Studio.}
 \label{img:bigquery-example-table}
\end{center}
\end{figure}

Prehľadávaním týchto dát a sledovaním obsahu HTTP hlavičiek je možné dopracovať sa k HTTP odpovediam s pravidlami technológie NEL.
Každú doménu, ktorá vo svojich odpovediach zaslala hlavičky \code{NEL} a \code{Report-To} je možné skúmať ako doménu s nasadeným monitorovaním NEL.

\pagebreak

\subsection{Poplatky za používanie}
\label{httparchive-costs}

Čo sa platieb týka, BigQuery pre užívateľov poskytuje bezplatný plán s nastavenými limitmi pre využívanie konkrétnych funkcií.
\textbf{Bezplatný plán} zahŕňa 1TB procesnej kapacity dát a 10GB úložného priestoru pre vlastné dáta, pričom dochádza každý mesiac \mbox{k obnove týchto bezplatných zdrojov}.
Po prečerpaní kapacity uvedenej v tomto pláne je nutné akékoľvek ďalšie operácie doplatiť.
Zoznam spôsobov, akými je možné zaplatiť za navýšenie spomenutých kapacít je rozsiahly, no pre prípady použitia tejto práce je relevantný platobný plán zvaný On-demand.
\textbf{On-demand plán}, alebo platba podľa potreby sa vzťahuje na procesnú kapacitu, ktorá sa vyčerpáva vykonávaním operácií nad dátami.
Cena za 1TB kapacity je v čase písania práce \$6.25, pričom stále platí, že prvý terabajt je každý mesiac zadarmo \cite{google-bq-pricing}.

\subsection{Výhody a nevýhody}

V prípade dát poskytovaných projektom HTTP Archive je zásadnou výhodou ich využitia fakt, že poskytujú náhľad do historického vývoja v nasadení technológie NEL.
Taktiež, keďže sú už všetky dáta dostupné na službe BigQuery, analýza je urýchlená o dĺžku procesu zaobstarávania si vlastných dát.
Ďalej, obsiahnuté dáta dosahujú veľmi vysoký objem, a teda je možné na základe nich vykonať rozsiahlu analýzu.

Avšak, na rozdiel od využitia vlastných dát, využitie dát projektu HTTP Archive je spoplatnené.
To znamená, že aj keď je možné vykonať rozsiahlu analýzu, platí, že čím väčší je jej rozsah, tým väčší je aj potrebný poplatok na jej vykonanie.