\chapter{Zdroje dát potrebných pre analýzu}
\label{data-sources-available-for-research}

Rozstrel - OK
Domains  -
Spec     - 
GC       -
BQ and SQL     -
HAR Tables   -
Costs        -
Alternatives -
Selenium     - 


Pre účely analýzy využívania technológie NEL je nutné nejakým spôsobom získať pohľad do diania vo verejnom internete.
Cieľom je pozerať sa na reálne komunikácie, ktoré buď už prebehli, alebo skúmať ako určité služby dostupné na internete
odpovedajú na HTTP(S) požiadavky zaslané v reálnom čase. V tejto kapitole sú do detailu rozvedené dostupné zdroje dát 
a ktoré z nich sú reálne aj použité v praktickej časti tejto práce -- kapitole \ref{analysis-and-its-results}.

\section{Potrebné dáta}

To čo pre analýzu v tejto práci bude potrebné zaobstarať sú hlavne konkrétne zdroje dát, z ktorých možno čerpať. Čo sa týka priamo toho,
čo by mali obsahovať, v ideálnom prípade by išlo o presný výpis všetkých webových domén dostupných verejne na internete 
a prípadne aj ich subdomén, ktoré možno skúmať (1). Takýto zoznam je potrebný práve preto aby sme jednak vedeli, ktoré domény
je vôbec možné analyzovať a ďalej preto, aby sme vo výsledkoch práce vedeli s touto informáciou pracovať a spájať 
relevantné vzťahy ako napríklad medzi jednotlivou doménou a jej vlastníkom. Práve touto spojitosťou sa postupne môžeme dostať napríklad 
o prehľade o tom, kto vlastní najvyšší počet domén využívajúcich NEL. 

Takýchto zoznamov existuje hneď niekoľko a sú spomenuté v ďalšej podkapitole, kde sú rozvedené do detailu aj ich dôležité vlastnosti.

Následne, keď už je známe, ktoré domény je možné podrobiť analýze, je nutné k nim nejako prideliť aj dáta týkajúce sa ich reálneho
sieťového prenosu. Keďže je tu zameraním technológia NEL, pod sieťovým prenosom je myslený menovite protokol HTTP(S), v ktorom
majú byť preverené prítomnosti hlavičiek spojazdňujúcich NEL (2). Existuje 
celá sada metód, ako sa k tomuto prenosu dostať. Z hľadiska prítomnosti, a teda aktuálneho stavu na webe je možné použiť 
takzvaný \textbf{Web Crawling}. \todo{web crawling basic info}. Na účel Web Crawlingu je možné použiť už existujúce technológie 
ako napríklad \textbf{Selenium Web Driver}. V rámci toho je vhodné Selénium priblížiť viac a práve tomu sa venuje podkapitola \todo{ref}.
No aby bol nadobudnutý celkový prehľad, užitočné je nahliadnuť taktiež do histórie prevádzky webu. V tomto bode je zasa nutné hľadať už
spracované a uložené dáta získané či už Web Crawlingom, alebo inými spôsobmi. Vyhovujúcim sprostredkovávateľom takýchto historických dát
je napríklad \textbf{HTTP Archive}, ktorému sa práca venuje primárne, Ide o službu, ktorá zaznamenáva vývoj webu od roku 2010 a teda
jej vhodnosť sa potvrdzuje tým, že samotná špecifikácia skúmanej technológie v tejto práci bola publikovaná až v roku 2018.

V oblasti zdrojov historických dát sa nachádzajú aj určité alternatívy, ktoré je možné využiť, no bude im tu venovaná iba okrajová
pozornosť. Každá z nich ale za zmienku nepopierateľne stojí, a preto sú popísané v podkapitole \todo{ref}.


\section{TRANCO}

Ako už bolo spomenuté (1), vzniká potreba zaobstarania si zoznamu domén, s ktorými možno pracovať. Čo sa vlastností takýchto domén týka,
mali by vystavovať verejnému internetu službu, ktorá je dosiahnuteľná, komunikuje protokolom HTTP, a ideálne je aj často navštevovaná,
a tým pádom relevantná pre verejnosť. Najviac teda prichádza do úvahy zamerať sa na získanie zoznamu \textbf{web stránok}, zoradeného podľa
ich popularity (návštevnosti), ktorý bude zároveň nadobúdať rozumnú veľkosť pre účely prieskumu. 

Zaobstarávaniu zoznamov web stránok, ktoré majú spĺňať konkrétne predurčené kritéria sa venuje rad iných existujúcich prác/služieb 
\cite{tranco}\cite{hacker-target-website-lists-overview}. Táto práca namiesto implementovania vlastného riešenia využíva jedno z nich -- TRANCO.

TRANCO je zoznam webových stránok zoradených podľa hodnotenia ich populárnosti/navštevovanosti, ktorý je odolný proti manipuláciam a vhodný na výskumné účely. \cite{tranco-homepage} 
Vznikol na základe častej potreby pre skúmanie práve takýchto stránok či už pre samotnú referenciu, alebo ako podklad pre ďalľí prieskum,
Príkladom jeho využitia môže byť analýza využitia konkrétnych webových technológií na týchto stránkach -- od použitej metódy komprimácie po samotné rámce, pomocou ktorých je 
stránka vyvinutá. TRANCO nie je prvým takýmto zoznamom, ale je prvým, ktorý sa zaoberá nedostatkami jeho predchodcov a spája stránky uvedené v nich do jednotného zoznamu, 
ktorý je stabilnejší, reprezentuje stránky v globálnej škále a dokonca do určitej miery odstraňuje stránky s potencionálne nebezpečným obsahom (napríklad phishing). \cite{tranco} 

\subsection{Generovanie zoznamu TRANCO}

V jeho štandardnej forme, zoznam TRANCO sa generuje každý deň. Takáto štandardná forma má svoje nastavenia zdrojov a dátumového ohraničenia relevancie dát, 
pravidiel filtrovania stránok a ich cieľový počet na zahrnutie do výsledného zoznamu. \cite{tranco-config}
Je taktiež možné zažiadať o vygenerovanie zoznamu s vlastným nastavením spomínaných podrobností. Je to možné uskutočniť na oficiálnej stránke TRANCO \todo{link at the bottom reference},
kde si užívateľ môže vyskladať žiadanú konfiguráciu a vygenerovať podľa nej nový zoznam. 

Pre oba prípady generovania nového zoznamu sa spolu s nimi vytvorí na oficiálnom webe TRANCO jedinečná stránka obsahujúca odkaz na stiahnutie výsledného zoznamu a citáciu, 
ktorou je možné jedinečne referencovať tento zoznam v prácach, ktoré ho použijú na svoje účely. 

\subsection{Zdroje dát pre generovanie}

Ako už bolo zmienené vyššie, stratégia získavania stránok, ktoré následne TRANCO podrobí hodnoteniu spočíva na vyťahovaní stránok z už existujúcich zoznamov s hotovým hodnotením. 
Zdrojom dát pre hodnotenie stránok TRANCO je teda sada niekoľkých podobných zoznamov, ktoré sami na ich úrovni používajú roôžne stratégie pre obstarávanie stránok a ich zoraďovanie 
podľa hodnotenia navštevovanosti. \cite{tranco-methodology} V tejto podkapitole sú popísané zainteresované zoznamy, z ktorých možno dáta kombinovať 
a na základe ktorých je možné nový zoznam vygenerovať.

\subsubsection{Alexa}

Popis
Ich zdroje
Vypocet finalneho rebricka
Plusy Minusy
Cennik

\subsubsection{Cisco Umbrella}

...
+ Porovnanie s predoslymi

\subsubsection{Majestic}
...

\subsubsection{Quantcast}
...

\subsubsection{Zhrnutie}

Moz is a search engine optimization service (SEO). They have a large data set of search related data. Using this, Moz makes available the top 500 sites for Free.
DomCop

\subsection{Možnosti konfigurácie generovania}

\todo{json s konfiguráciou}


\section{HTTP Archive}

Zdroje dát, ich umiestnenie, formát, veľkosť, pokrytie, vhodnosť.
\\
Spôsoby ich získavania (možné a zvolené) sú popísané v kapitole \ref{possible-analysis-strategies} \textbf{???}
\\
GCP intro. Iba čo to je, čo ponúka, na ktorú sekciu sa budeme zameriavať a používať
\\
Non-profit project - sponsors keep it alive

https://almanac.httparchive.org/en/2021/methodology
Google Cloud Locations, USA
Desktop websites are run from within a desktop Chrome environment on a Linux VM. The network speed is equivalent to a cable connection.

Mobile websites are run from within a mobile Chrome environment on an emulated Moto G4 device with a network speed equivalent to a 3G connection.

Web Performance Working Group
HAR files store session information in plain text file using the popular JSON format. The specifications for HAR file format were produced by the Web Performance Group of the World Web Consortium (W3C) but could not be published. However, it successfully defined an archival format for HTTP transactions.

WebPageTest ??

HTTPArchive Lens - "A lens focuses on a specific subset of websites. Through a lens, you'll see data about those particular websites only. For example, the WordPress lens focuses only on websites that are detected as being built with WordPress. We use \textbf{Wappalayzer} to detect over 1,000 web technologies and choose a few interesting ones to become lenses."

https://docs.fileformat.com/web/har/

\todo{Internet Archive 1996 ?}

All pages are tested with an empty cache in a logged out state, which may not reflect how real users would access them.

Lighthouse is used to run audits against the page to analyze its quality in areas like accessibility and SEO. The Tools section below goes into each of these tools in more detail.

HTTP Archive je open source projekt, ktorého cieľom je získavať a skladovať dáta 
týkajúce sa stavu webu, ktoré potom sprístupňujú verejnosti tak, aby ich mohla
použiť na rôzne účely. V rámci získavania sa prehľadávajú milióny URLs (Uniform 
Resource Locators) na ako počítačových zariadeniach, tak aj na mobiloch, kde v oboch prípadoch sa na tento účel používa webový prehliadač Google Chrome. 
\todo{cite} Tieto URLs projekt HTTP Archive čerpá z 
\textbf{Chrome User Experience Report}-u, čo je sada dát týkajúcich sa výkonnosti
prehliadania?? od reálnych užívateľov pre najpopulárnejšie stránky na webe???
\todo{cite}. Tieto dáta, ktoré sa do súčasnosti nahromadili sú vo svojej podstate popisom histórie a vývoja webu v čase od založenia HTTP Archive v roku 2010.

Proces získavania dát sa začína vždy prvý deň každého mesiaca tým, že sa vezmú URLs uvedené vyššie a vložia sa ako vstup do inštancie nástroja 
\textbf{Webpagetest}, ktorý je zodpovedný za samotné simulovanie komunikácie s 
každou URL. Takéto testy sú vykonávané v prostredí prehliadača Google Chrome.
Vždy sa načíta práve jedna URL a zašle sa na ňu najprv prvá HTTP(S) požiadavka pre zbieranie obyčajné metriky, a potom znova za použitia čistého prehliadacieho profilu v nástroji \textbf{Lighthouse} pre hĺbkovejšiu analýzu spojenia (pre viac info o nástroji Lighthouse zatiaľ pozri sem - \todo{co s tymto zas}. Výsledkom takéhoto testovacieho spojenia sú dáta, ktoré sa ukladajú vo formáte \textbf{HAR} (Http ARchive formát). Tieto HAR súbory sú potom prevzaté iným nástrojom od HTTP Archive, spracované a jedlotlivé položky sa postupne naplnia do tabuliek v BigQuery \todo{odkaz na vysvetlivku}.

Tento proces sa nazýva \textbf{Web Crawling}

\subsection{Google Cloud Platform}

One of the great things about the BigQuery platform is how well it handles massive datasets and queries that can be incredibly computationally intensive

KJerabek Repository

At the time of this writing, the free tier allows 10GB of storage and 1TB of data processing per month

\textbf{Types of GCP BQ HTTP Archive tables}
\\
Some of the types of tables you'll find useful when getting started are described below. These table names all follow the format yyyy\textunderscore mm\textunderscore dd\textunderscore desktop and yyyy\textunderscore mm\textunderscore dd\textunderscore mobile.

Detekcia kompresie. Detekcia chýb...

\section{Alternatívy k HTTP Archive}

\section{Teória k čomukoľvek ďalšiemu, čo sa bude používať}

Našiel som napríklad \textbf{crawler.ninja} - web crawled data od 09.05.2021 do 1.10.2023 dostupné na stiahnutie (ale zdroj sú asi stále HTTP Archive dáta)

\subsection{Nástroje pre automatickú analýzu stavu nasadenia NEL - teória (selenium, bs4, pandas...)}

Selenium teória apod.
