\chapter{Zdroje dát potrebných pre analýzu}
\label{data-sources-available-for-research}

Rozstrel - OK
Domains  -
Spec     - 
GC       -
BQ and SQL     -
HAR Tables   -
Costs        -
Alternatives -
Selenium     - 


Pre účely analýzy využívania technológie NEL je nutné nejakým spôsobom získať pohľad do diania vo verejnom internete.
Cieľom je pozerať sa na reálne komunikácie, ktoré buď už prebehli, alebo skúmať ako určité služby dostupné na internete
odpovedajú na HTTP(S) požiadavky zaslané v reálnom čase. V tejto kapitole sú do detailu rozvedené dostupné zdroje dát 
a ktoré z nich sú reálne aj použité v praktickej časti tejto práce -- kapitole \ref{analysis-and-its-results}.

\section{Potrebné dáta}

To čo pre analýzu v tejto práci je potrebné zaobstarať sú hlavne konkrétne zdroje dát, z ktorých možno čerpať. Čo sa týka priamo toho,
čo by mali obsahovať, v ideálnom prípade by išlo o presný výpis všetkých webových domén dostupných verejne na internete 
a prípadne aj ich subdomén, ktoré možno skúmať (1). Takýto zoznam je potrebný práve preto aby sme jednak vedeli, ktoré domény
je vôbec možné analyzovať a ďalej preto, aby sme vo výsledkoch práce vedeli s touto informáciou pracovať a spájať 
relevantné vzťahy ako napríklad medzi jednotlivou doménou a jej vlastníkom. Práve touto spojitosťou sa postupne môžeme dostať napríklad 
o prehľade o tom, kto vlastní najvyšší počet domén využívajúcich NEL. 

Takýchto zoznamov existuje hneď niekoľko a sú spomenuté v ďalšej podkapitole, kde sú rozvedené do detailu aj ich dôležité vlastnosti.

Následne, keď už je známe, ktoré domény je možné podrobiť analýze, je nutné k nim nejako prideliť aj dáta týkajúce sa ich reálneho
sieťového prenosu. Keďže je tu zameraním technológia NEL, pod sieťovým prenosom je myslený menovite protokol HTTP(S), v ktorom
majú byť preverené prítomnosti hlavičiek spojazdňujúcich NEL (2). Existuje 
celá sada metód, ako sa k tomuto prenosu dostať. Z hľadiska prítomnosti, a teda aktuálneho stavu na webe je možné použiť 
takzvaný \textbf{Web Crawling}. \todo{web crawling basic info}. Na účel Web Crawlingu je možné použiť už existujúce technológie 
ako napríklad \textbf{Selenium Web Driver}. V rámci toho je vhodné Selénium priblížiť viac a práve tomu sa venuje podkapitola \todo{ref}.
No aby bol nadobudnutý celkový prehľad, užitočné je nahliadnuť taktiež do histórie prevádzky webu. V tomto bode je zasa nutné hľadať už
spracované a uložené dáta získané či už Web Crawlingom, alebo inými spôsobmi. Vyhovujúcim sprostredkovávateľom takýchto historických dát
je napríklad \textbf{HTTP Archive}, ktorému sa práca venuje primárne, Ide o službu, ktorá zaznamenáva vývoj webu od roku 2010 a teda
jej vhodnosť sa potvrdzuje tým, že samotná špecifikácia skúmanej technológie v tejto práci bola publikovaná až v roku 2018.

V oblasti zdrojov historických dát sa nachádzajú aj určité alternatívy, ktoré je možné využiť, no bude im tu venovaná iba okrajová
pozornosť. Každá z nich ale za zmienku nepopierateľne stojí, a preto sú popísané v podkapitole \todo{ref}.


\section{TRANCO}

Ako už bolo spomenuté (1), vzniká potreba zaobstarania si zoznamu domén, s ktorými možno pracovať. Čo sa vlastností takýchto domén týka,
mali by vystavovať verejnému internetu službu, ktorá je dosiahnuteľná, komunikuje protokolom HTTP, a ideálne je aj často navštevovaná,
a tým pádom relevantná pre verejnosť. Najviac teda prichádza do úvahy zamerať sa na získanie zoznamu \textbf{web stránok}, zoradeného podľa
ich popularity (návštevnosti), ktorý bude zároveň nadobúdať rozumnú veľkosť pre účely prieskumu. 

Zaobstarávaniu zoznamov web stránok, ktoré majú spĺňať konkrétne predurčené kritéria sa venuje rad iných existujúcich prác/služieb 
\cite{tranco}\cite{hacker-target-website-lists-overview}. Táto práca namiesto implementovania vlastného riešenia využíva jedno z nich -- TRANCO.

TRANCO je zoznam webových stránok zoradených podľa hodnotenia ich populárnosti/navštevovanosti, ktorý je odolný proti manipuláciam a vhodný na výskumné účely. \cite{tranco-homepage} 
Vznikol na základe častej potreby pre skúmanie práve takýchto stránok či už pre samotnú referenciu, alebo ako podklad pre ďalľí prieskum,
Príkladom jeho využitia môže byť analýza využitia konkrétnych webových technológií na týchto stránkach -- od použitej metódy komprimácie po samotné rámce, pomocou ktorých je 
stránka vyvinutá. TRANCO nie je prvým takýmto zoznamom, ale je prvým, ktorý sa zaoberá nedostatkami jeho predchodcov a spája stránky uvedené v nich do jednotného zoznamu, 
ktorý je stabilnejší, reprezentuje stránky v globálnej škále a dokonca do určitej miery odstraňuje stránky s potencionálne nebezpečným obsahom (napríklad phishing). \cite{tranco} 

\subsection{Generovanie zoznamu TRANCO}

V jeho štandardnej forme, zoznam TRANCO sa generuje každý deň. Takáto štandardná forma má svoje nastavenia zdrojov a dátumového ohraničenia relevancie dát, 
pravidiel filtrovania stránok a ich cieľový počet na zahrnutie do výsledného zoznamu. \cite{tranco-config}
Je taktiež možné zažiadať o vygenerovanie zoznamu s vlastným nastavením spomínaných podrobností. Je to možné uskutočniť na oficiálnej stránke TRANCO \todo{link at the bottom reference},
kde si užívateľ môže vyskladať žiadanú konfiguráciu a vygenerovať podľa nej nový zoznam. 

Pre oba prípady generovania nového zoznamu sa spolu s nimi vytvorí na oficiálnom webe TRANCO jedinečná stránka obsahujúca odkaz na stiahnutie výsledného zoznamu a citáciu, 
ktorou je možné jedinečne referencovať tento zoznam v prácach, ktoré ho použijú na svoje účely. 

\subsection{Zoraďovacie zoznamy použité v TRANCO}

Ako už bolo zmienené vyššie, stratégia získavania stránok, ktoré následne TRANCO podrobí hodnoteniu spočíva na vyťahovaní stránok z už existujúcich zoznamov s hotovým hodnotením. 
Zdrojom dát pre hodnotenie stránok TRANCO je teda množina niekoľkých podobných zoznamov, ktoré sami na ich úrovni používajú rôžne stratégie pre obstarávanie stránok a ich zoraďovanie 
podľa hodnotenia navštevovanosti. \cite{tranco-methodology} V tejto podkapitole sú popísané zainteresované zoznamy, z ktorých možno dáta kombinovať 
a na základe ktorých je možné nový zoznam vygenerovať.

\subsubsection{Alexa}

Alexa, dcérska spoločnosť Amazon.com, publikovala na svojich stránkach od decembra 2008 až po začiatok augusta 2023 zoznam \textbf{Alexa top sites}, v ktorom zoraďovala 1 milión najnavšetevovanejších webstránok. \cite{tranco-methodology}
Zdrojom dát pre jeho tvorbu bolo rozšírenie pre webové prehliadače, ktoré si užívatelia mohli stiahnuť a po jeho inštalácií na vybranom prehliadači začalo zbierať a odosielať
dáta o prehliadaní internetu do Alexy. Pre účely vytvorenia hodnotenia stránok sa teda využíva sieťová prevádzka priamo vedená cez protokol HTTP(S). Počet užívateľov s nainštalovaným rozšírením bol však obmedzený 
a to mohlo spôsovovať značné skreslenie celkových výsledkov kvôli malému zastúpeniu vzoriek prehliadania z celkovej sady -- všetkých používateľov internetu. Avšak, existujú zmienky, 
kde sa cituje pôvodná oficiálna stránka tohto zoznamu, kde autori tvrdili, že počet týchto užívateľov sa pohyboval v ráde niekoľkých miliónov. \cite{tranco}

Svoje hodnotenie Alexa zakladala na dvoch základných metrikách: \cite{kinsta-alexa-rank-article}\cite{tranco}
\begin{itemize}
    \item Počet návštevníkov stránky za daný deň (viac návštev od jedného sa počíta ako jediná návšteva)
    \item Spriemerovaný počet otvorení hocijakej podstránky (URL v rámci sledovanej domény)
\end{itemize}

Zo spomenutých metrík má vyššiu váhu pri rozhodovaní o popularite práve počet návštevníkov stránky za daný deň, čož môžeme podľa jej popisu nazvať aj počet unikátnych návštev za deň. \cite{tranco}
Obe metriky sa pre špecifický deň následne spriemerovali so všetkými takto napočítanými hodnotami za posledné tri mesiace, aby zoznam nadobudol určitú mieru stability. 
Tým pádom sú do úvahy pre každodenný zoznam vzané aj aktuálne trendy v prehliadaní. Vo výsledku sa tým predchádza nepresnému celkovému hodnoteniu popularity v prípadoch, 
kedy by neobvyklý jednorazový nárast v popularite doposiaľ neznámej stránky mal predbehnúť zaradenie inej, dlhodobo populárnej a vysoko zaradenej stránky v zozname. 
Koncom roka 2016 bol na nejakú dobu odstavený z prevádzky pre verejnosť na hlavných stránkach Alexy a znova bol sprístupnený pod záštitou Amazon Web Services ako platená služba \textbf{Alexa Top Sites}. 
Neskôr, začiatkom roka 2018 sa zhoršila jeho celková stabilita z toho dôvodu, že jeho algoritmus vyhodnocovania zaradenia stránok prestal brať ohľad na priemerovanie dát za predošlé mesiace.
Od vtedy, konkrétne 30.01.2018, bol zhotovovaný striktne iba za každý konkrétny deň \cite{tranco} až dokým táto služba nebola nadobro prerušená 01.08.2023, čo z nej teraz robí \textbf{zastaraný zoznam} \cite{tranco-methodology}.


Ake domeny rata alexa
Tranco dve metody urcenia vahy zaradenia

\subsubsection{Majestic Million}

Majestic Million je služba, ktorá rovnako ako Alexa poskytuje zoznam webových domien zoradených podľa množstva spätných odkazov na ne ukazujúcich. \cite{majestic-million-homepage}\cite{majestic-million-ranking}
Ako individuálne technologické riešenie bolo uverejnené na webe Majestic 1.10.2012. \cite{majestic-million-publication}
V jednoduchosti to znamená, že čím viac priamych odkazov na danú doménu sa na webe nájde, tým vyššie bude v tomto zozname zaradená. 
Majestic Million je vyvinutý spoločnosťou Majestic, ktorá sa zaoberá SEO (Search Engine Optimization) a skúmaním práve takýchto prepojení medzi doménami.
Majestic na dennej báze prehľadáva globálne zhruba 450 miliárd URL v rámci verejného internetu aby svoj zoznam zostavil. 
Hodnotenie je založené na počte spätných odkazov smerujúcich na jednotlivé domény. Tieto spätné odkazy, iným názvom \textbf{backlinks}, sú vlastne podsiete triedy C (IPv4 /24), ktoré odkazujú
na hodnotenú doménu minimálne jedenkrát.
Okrem každodenného vyhodnocovania aktuálneho zaradenia domén sa taktiež berie ohľad aj na priemer ich hodnotenia za posledných 120 dní. \cite{tranco-methodology}
Okrem iného je tiež vhodné spomenúť, že autori na svojom webe označujú vysoké zaradenie domény v tomto zozname za znak dôveryhodnosti. \cite{majestic-million-homepage}

V rámci celkového výstupného zoznamu Majestic Million sa berú do úvahy domény, \textbf{ale aj ich sub-domény}, takže sa môže stať, že napríklad doména \code{google.com} sa v ňom bude vyskytovať viackrát v podobe 
svojich sub-domén (\code{play.google.com}, \code{maps.google.com}, \code{mail.google.com} a iné). \cite{majestic-million-sub-domain-filtered}

Na oficiálnom webe Majestic sú taktiež dostupné doplnkové služby, ktoré svojim užívateľom poskytujú naväzujúce užitočné informácie o týchto doménach. 
Jednou z nich je napríklad možnosť bližšie porovnať až do 10 domén, kde vstupom sú ich mená a výstupom je prehľadná tabuľka (takzvaná Buzz Table) zobrazujúca ich zaradenie a iné detaily, medzi ktoré patrí aj počet domén
referujúcich na tie vstupné a aj celkový počet ich externých backlinkov. \cite{majestic-million-homepage}
Existujú rôzne ďalšie služby, ktoré Majestic ponúka, no až na samotný rebríček a spomenutý Buzz Table sú tie ostatné spoplatnené, kde najlacnejšia možnosť platobného plánu začína na USD 46.99 mesačne. \cite{majestic-million-pricing}


\subsection{Možnosti konfigurácie generovania}

\todo{json s konfiguráciou}

\subsubsection{Zhrnutie}

Moz is a search engine optimization service (SEO). They have a large data set of search related data. Using this, Moz makes available the top 500 sites for Free.
DomCop

Na základe predošlého textu v tejto kapitole skúsiť dorozprávať prípeh o výbere a využiteľnosťi rebríčka TRANCO


\section{HTTP Archive}

Zdroje dát, ich umiestnenie, formát, veľkosť, pokrytie, vhodnosť.
\\
Spôsoby ich získavania (možné a zvolené) sú popísané v kapitole \ref{possible-analysis-strategies} \textbf{???}
\\
GCP intro. Iba čo to je, čo ponúka, na ktorú sekciu sa budeme zameriavať a používať
\\
Non-profit project - sponsors keep it alive

https://almanac.httparchive.org/en/2021/methodology
Google Cloud Locations, USA
Desktop websites are run from within a desktop Chrome environment on a Linux VM. The network speed is equivalent to a cable connection.

Mobile websites are run from within a mobile Chrome environment on an emulated Moto G4 device with a network speed equivalent to a 3G connection.

Web Performance Working Group
HAR files store session information in plain text file using the popular JSON format. The specifications for HAR file format were produced by the Web Performance Group of the World Web Consortium (W3C) but could not be published. However, it successfully defined an archival format for HTTP transactions.

WebPageTest ??

HTTPArchive Lens - "A lens focuses on a specific subset of websites. Through a lens, you'll see data about those particular websites only. For example, the WordPress lens focuses only on websites that are detected as being built with WordPress. We use \textbf{Wappalayzer} to detect over 1,000 web technologies and choose a few interesting ones to become lenses."

https://docs.fileformat.com/web/har/

\todo{Internet Archive 1996 ?}

All pages are tested with an empty cache in a logged out state, which may not reflect how real users would access them.

Lighthouse is used to run audits against the page to analyze its quality in areas like accessibility and SEO. The Tools section below goes into each of these tools in more detail.

HTTP Archive je open source projekt, ktorého cieľom je získavať a skladovať dáta 
týkajúce sa stavu webu, ktoré potom sprístupňujú verejnosti tak, aby ich mohla
použiť na rôzne účely. V rámci získavania sa prehľadávajú milióny URLs (Uniform 
Resource Locators) na ako počítačových zariadeniach, tak aj na mobiloch, kde v oboch prípadoch sa na tento účel používa webový prehliadač Google Chrome. 
\todo{cite} Tieto URLs projekt HTTP Archive čerpá z 
\textbf{Chrome User Experience Report}-u, čo je sada dát týkajúcich sa výkonnosti
prehliadania?? od reálnych užívateľov pre najpopulárnejšie stránky na webe???
\todo{cite}. Tieto dáta, ktoré sa do súčasnosti nahromadili sú vo svojej podstate popisom histórie a vývoja webu v čase od založenia HTTP Archive v roku 2010.

Proces získavania dát sa začína vždy prvý deň každého mesiaca tým, že sa vezmú URLs uvedené vyššie a vložia sa ako vstup do inštancie nástroja 
\textbf{Webpagetest}, ktorý je zodpovedný za samotné simulovanie komunikácie s 
každou URL. Takéto testy sú vykonávané v prostredí prehliadača Google Chrome.
Vždy sa načíta práve jedna URL a zašle sa na ňu najprv prvá HTTP(S) požiadavka pre zbieranie obyčajné metriky, a potom znova za použitia čistého prehliadacieho profilu v nástroji \textbf{Lighthouse} pre hĺbkovejšiu analýzu spojenia (pre viac info o nástroji Lighthouse zatiaľ pozri sem - \todo{co s tymto zas}. Výsledkom takéhoto testovacieho spojenia sú dáta, ktoré sa ukladajú vo formáte \textbf{HAR} (Http ARchive formát). Tieto HAR súbory sú potom prevzaté iným nástrojom od HTTP Archive, spracované a jedlotlivé položky sa postupne naplnia do tabuliek v BigQuery \todo{odkaz na vysvetlivku}.

Tento proces sa nazýva \textbf{Web Crawling}

\subsection{Google Cloud Platform}

One of the great things about the BigQuery platform is how well it handles massive datasets and queries that can be incredibly computationally intensive

KJerabek Repository

At the time of this writing, the free tier allows 10GB of storage and 1TB of data processing per month

\textbf{Types of GCP BQ HTTP Archive tables}
\\
Some of the types of tables you'll find useful when getting started are described below. These table names all follow the format yyyy\textunderscore mm\textunderscore dd\textunderscore desktop and yyyy\textunderscore mm\textunderscore dd\textunderscore mobile.

Detekcia kompresie. Detekcia chýb...

\section{Alternatívy k HTTP Archive}

\section{Teória k čomukoľvek ďalšiemu, čo sa bude používať}

Našiel som napríklad \textbf{crawler.ninja} - web crawled data od 09.05.2021 do 1.10.2023 dostupné na stiahnutie (ale zdroj sú asi stále HTTP Archive dáta)

\subsection{Nástroje pre automatickú analýzu stavu nasadenia NEL - teória (selenium, bs4, pandas...)}

Selenium teória apod.
