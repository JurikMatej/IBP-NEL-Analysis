\chapter{Zdroje dát potrebných pre analýzu}
\label{data-sources-available-for-research}

Rozstrel - OK
Domains  -
Spec     - 
GC       -
BQ and SQL     -
HAR Tables   -
Costs        -
Alternatives -
Selenium     - 


Pre účely analýzy využívania technológie NEL je nutné nejakým spôsobom získať pohľad do diania vo verejnom internete.
Cieľom je pozerať sa na reálne komunikácie, ktoré buď už prebehli, alebo skúmať ako určité služby dostupné na internete
odpovedajú na HTTP(S) požiadavky zaslané v reálnom čase. V tejto kapitole budú do detailu rozvedené dostupné zdroje dát 
a ktoré z nich sú reálne aj použité v praktickej časti tejto práce -- kapitole \ref{analysis-and-its-results}.

\section{Potrebné dáta}

To čo pre analýzu v tejto práci bude potrebné zaobstarať sú hlavne konkrétne zdroje dát, z ktorých možno čerpať. Čo sa týka priamo toho,
čo by mali obsahovať, v ideálnom prípade by išlo o presný výpis všetkých webových domén dostupných verejne na internete 
a prípadne aj ich subdomén, ktoré možno skúmať. Takýto zoznam je potrebný práve preto aby sme jednak vedeli, ktoré domény
je vôbec možné analyzovať a ďalej preto, aby sme vo výsledkoch práce vedeli s touto informáciou pracovať a nadväzovať 
relevantné spojitosti ako napríklad medzi jednotlivou doménou a jej vlastníkom. 

Takýchto zoznamov existuje hneď niekoľko a budú spomenuté v ďalšej podkapitole, kde sú rozvedené do detailu aj ich dôležité vlastnosti.

Následne, keď už je známe, ktoré domény je možné podrobiť analýze, je nutné k nim nejako prideliť aj dáta týkajúce sa ich reálneho
sieťového prenosu. Keďže je tu zameraním technológia NEL, pod sieťovým prenosom je myslený menovite protokol HTTP(S). Existuje 
celá sada metód, ako sa k tomuto prenosu dostať. Z hľadiska prítomnosti, a teda aktuálneho stavu na webe je možné použiť 
takzvaný \textbf{Web Crawling}. \todo{web crawling basic info}. Na účel Web Crawlingu je možné použiť už existujúce technológie 
ako napríklad \textbf{Selenium Web Driver}. V rámci toho je vhodné Selénium priblížiť viac a práve tomu sa venuje podkapitola \todo{ref}.
No aby bol nadobudnutý celkový prehľad, užitočné je nahliadnuť taktiež do histórie prevádzky webu. V tomto bode je zasa nutné hľadať už
spracované a uložené dáta získané či už Web Crawlingom, alebo inými spôsobmi. Vyhovujúcim sprostredkovávateľom takýchto historických dát
je napríklad \textbf{HTTP Archive}, ktorému sa práca venuje primárne, Ide o službu, ktorá zaznamenáva vývoj webu od roku 2010 a teda
jej vhodnosť sa potvrdzuje tým, že samotná špecifikácia skúmanej technológie v tejto práci bola publikovaná až v roku 2018.

V oblasti zdrojov historických dát sa nachádzajú aj určité alternatívy, ktoré je možné využiť, no bude im tu venovaná iba okrajová
pozornosť. Každá z nich ale za zmienku nepopierateľne stojí, a preto budú popísané v podkapitole \todo{ref}.

\section{Zoznamy skúmateľných domén}

Domény určené na analýzu - Zistiť ktoré zoznamy domén použijeme a potom spísať niečo aj o nich
\\
Alexa TOP 1 milion (first 500 000?)

\todo{write this}

\section{HTTP Archive}

Zdroje dát, ich umiestnenie, formát, veľkosť, pokrytie, vhodnosť.
\\
Spôsoby ich získavania (možné a zvolené) budú popísané v kapitole \ref{possible-analysis-strategies} \textbf{???}
\\
GCP intro. Iba čo to je, čo ponúka, na ktorú sekciu sa budeme zameriavať a používať
\\
Non-profit project - sponsors keep it alive

https://almanac.httparchive.org/en/2021/methodology
Google Cloud Locations, USA
Desktop websites are run from within a desktop Chrome environment on a Linux VM. The network speed is equivalent to a cable connection.

Mobile websites are run from within a mobile Chrome environment on an emulated Moto G4 device with a network speed equivalent to a 3G connection.

Web Performance Working Group
HAR files store session information in plain text file using the popular JSON format. The specifications for HAR file format were produced by the Web Performance Group of the World Web Consortium (W3C) but could not be published. However, it successfully defined an archival format for HTTP transactions.

WebPageTest ??

HTTPArchive Lens - "A lens focuses on a specific subset of websites. Through a lens, you'll see data about those particular websites only. For example, the WordPress lens focuses only on websites that are detected as being built with WordPress. We use \textbf{Wappalayzer} to detect over 1,000 web technologies and choose a few interesting ones to become lenses."

https://docs.fileformat.com/web/har/

\todo{Internet Archive 1996 ?}

All pages are tested with an empty cache in a logged out state, which may not reflect how real users would access them.

Lighthouse is used to run audits against the page to analyze its quality in areas like accessibility and SEO. The Tools section below goes into each of these tools in more detail.

HTTP Archive je open source projekt, ktorého cieľom je získavať a skladovať dáta 
týkajúce sa stavu webu, ktoré potom sprístupňujú verejnosti tak, aby ich mohla
použiť na rôzne účely. V rámci získavania sa prehľadávajú milióny URLs (Uniform 
Resource Locators) na ako počítačových zariadeniach, tak aj na mobiloch, kde v oboch prípadoch sa na tento účel používa webový prehliadač Google Chrome. 
\todo{cite} Tieto URLs projekt HTTP Archive čerpá z 
\textbf{Chrome User Experience Report}-u, čo je sada dát týkajúcich sa výkonnosti
prehliadania?? od reálnych užívateľov pre najpopulárnejšie stránky na webe???
\todo{cite}. Tieto dáta, ktoré sa do súčasnosti nahromadili sú vo svojej podstate popisom histórie a vývoja webu v čase od založenia HTTP Archive v roku 2010.

Proces získavania dát sa začína vždy prvý deň každého mesiaca tým, že sa vezmú URLs uvedené vyššie a vložia sa ako vstup do inštancie nástroja 
\textbf{Webpagetest}, ktorý je zodpovedný za samotné simulovanie komunikácie s 
každou URL. Takéto testy sú vykonávané v prostredí prehliadača Google Chrome.
Vždy sa načíta práve jedna URL a zašle sa na ňu najprv prvá HTTP(S) požiadavka pre zbieranie obyčajné metriky, a potom znova za použitia čistého prehliadacieho profilu v nástroji \textbf{Lighthouse} pre hĺbkovejšiu analýzu spojenia (pre viac info o nástroji Lighthouse zatiaľ pozri sem - \todo{co s tymto zas}. Výsledkom takéhoto testovacieho spojenia sú dáta, ktoré sa ukladajú vo formáte \textbf{HAR} (Http ARchive formát). Tieto HAR súbory sú potom prevzaté iným nástrojom od HTTP Archive, spracované a jedlotlivé položky sa postupne naplnia do tabuliek v BigQuery \todo{odkaz na vysvetlivku}.

Tento proces sa nazýva \textbf{Web Crawling}

\section{GCP BQ a HTTP Archive (+ iné zdroje v prípade potreby)}

One of the great things about the BigQuery platform is how well it handles massive datasets and queries that can be incredibly computationally intensive

KJerabek Repository

At the time of this writing, the free tier allows 10GB of storage and 1TB of data processing per month

\textbf{Types of GCP BQ HTTP Archive tables}
\\
Some of the types of tables you'll find useful when getting started are described below. These table names all follow the format yyyy\textunderscore mm\textunderscore dd\textunderscore desktop and yyyy\textunderscore mm\textunderscore dd\textunderscore mobile.

Detekcia kompresie. Detekcia chýb...

\section{Teória k čomukoľvek ďalšiemu, čo sa bude používať}

Našiel som napríklad \textbf{crawler.ninja} - web crawled data od 09.05.2021 do 1.10.2023 dostupné na stiahnutie (ale zdroj sú asi stále HTTP Archive dáta)

\section{Nástroje pre automatickú analýzu stavu nasadenia NEL - teória (selenium, bs4, pandas...)}

Selenium teória apod.
