\chapter{Analýza}

Po preskúmaní možností pre vykonanie podrobnej analýzy nasadenia NEL som podľa vypracovaného návrhu implementoval potrebné nástroje.
Úlohou týchto nástrojov je dopracovať sa k stanoveným cieľom mojej práce.  
Najskôr v tejto kapitole popisujem svoju prácu z hľadiska toho, čo bolo implementované.
Potom, v sekcií \ref{sec:results} prezentujem jej celkové výsledky.


\section{Implementácia nástrojov pre analýzu}

Potrebné nástroje podľa návrhu práce (viď kapitolu \ref{possible-analysis-strategies}) predstavujú implementované skripty pre dve rozdielne metódy analýzy:
\begin{enumerate}
    \item Práca s HTTP Archive:
    \begin{enumerate}
        \item získať \textit{historické} dáta, 
        \item analyzovať ich a vyprodukovať výsledné metriky,
        \item výsledky podľa potreby vhodne vizualizovať.
    \end{enumerate}

    \item Automatizované prehliadanie súčasného webu:
    \begin{enumerate}
        \item získať \textit{súčasné} dáta,
        \item analyzovať ich a vyprodukovať výsledné metriky, 
        \item výsledky podľa potreby vhodne vizualizovať.
    \end{enumerate}
\end{enumerate}

Skutočnosť, že sa tieto dve metódy odlišujú iba v spôsobe získavania vstupných dát som využil pri návrhu skriptov.
Na začiatok som teda definoval štruktúru výstupných dát z prvého kroku oboch metód.
Spoločná štrutúra dát umožňuje implementovať zvyšné dva kroky pre obe metódy rovnako.
Tabuľka \ref{tab:analysis-data-structure} v prílohách túto štruktúru popisuje. 
% Bližšie ju popisuje nasledujúca sekcia o implementácií nástroja pre HTTP Archive.

\subsection{Skript pre prácu s HTTP Archive}

Vzhľadom na to, že som sa rozhodol ako primárny zdroj dát použiť projekt HTTP Archive, začal som s implementáciou skriptu pre prácu práve s ním.
K vývoju som pristupoval tak, aby mi vo výsledku skript čo najviac uľahčil prácu pri opakovanom vyhodnocovaní zvolených metrík.
Stratégiu opakovaného vyhodnocovania metrík som zvolil z toho dôvodu, že som najskôr potreboval dôkladne otestovať optimalizácie získavania dát spomínané v návrhu práce.
Preto som v prvom rade začal vyhodnocovať iba čiastkové výsledky z malého objemu dát, teda niekoľkých mesiacov na začiatku skúmaného časového obdobia.
Jednorazové stiahnutie všetkých dát by totiž viedlo k spotrebovaniu prevažnej väčšiny dostupných finančných zdrojov pre prácu s historickými dátami.

Tieto prvé testy som vykonával pomocou skriptu prebratého od autorov predošlej analýzy (viď existujúcu analýzu v rámci návrhu v kapitole \ref{possible-analysis-strategies}).
Pri ich vykonávaní som objavil problémy v použití prebratého skriptu týkajúce sa mojej optimalizácie použitého príkazu GoogleSQL na extrahovanie dát.
Cieľom optimalizácie bolo rozšíriť vzorku dát pre každú skúmanú doménu.
Dôsledkom toho, že sa príkaz optimalizovať podarilo, sa objem dát na stiahnutie z BigQuery niekoľkokrát zvýšil.
Prebratý skript však používal knižnicu, ktorá nepodporovala sťahovanie dát s vysokým objemom.
To ma viedlo k vytvoreniu nového skriptu, ktorý sa týmto novým podmienkam prispôsobí.

\subsubsection{Špecifikácia}

Pôvodný, prebratý skript pre sťahovanie dát bol napísaný v jazyku Python3.
Keďže mojím cieľom bolo implementovať rovnakú funkcionalitu, no podporovať veľký objem dát, rozhodol som sa pre implementáciu v rovnakom jazyku.
Nový skript, \code{query\_and\_store.py} som napísal v jazyku Python3 s verziou \code{3.12.0}.
Pôvodný algoritmus bol spustiť GoogleSQL príkaz na BigQuery a stiahnuť jeho službou dočasne uložené výsledky.
Nedostal som sa k informácií o maximálnej veľkosti dočasných výsledkov, no z pozorovania som zistil, že zlyhávajú pokusy stiahnuť viac ako 100 megabajtov dát.
Zistil som, že vhodnou alternatívou k tomuto prístupu je vyexportovať výsledné dáta do úložiska Google Cloud Storage nazvaného \textit{bucket} (ďalej už len GCS a GCS bucket).
Pre službu GCS existuje knižnica \code{google.cloud.storage}, ktorá implementuje klienta pre operácie ako práve sťahovanie veľkých objemov dát z GCS bucketov.
Pomocou pôvodnej knižnice, \code{google.cloud.bigquery}, som implementoval rozhranie pre extrahovanie a export dát z BigQuery.
Za pomoci novej knižnice som implementoval rozhranie pre sťahovanie dát z GCS.

Výstupom pôvodného skriptu boli kompresované súbory Apache Parquet (súborová prípona \code{.parquet}), ktoré rovnako ako BigQuery ukladajú dáta formátované s orientáciou na stĺpce.
Keďže použitím nového skriptu výrazne narastá objem analyzovaných dát, schopnosť vyberať iba niektoré potrebné stĺpce pre výpočty konkrétnej metriky je kľúčová z hľadiska pamäťovej náročnosti.
Rozhodol som sa preto ponechať pôvodný formát výstupov aj pre nový skript. 
Na základe toho, že HTTP Archive uverejňuje svoje výsledky po mesiacoch, sú celkovým výstupom 
nového skriptu všetky relevantné informácie o zdrojoch za daný mesiac.
Tieto relevantné informácie sú definované ako polia v tabuľke \ref{tab:analysis-data-structure} v prílohách.
Množinu mesiacov, pre ktoré sa majú stiahnuť dáta, je možné definovať v konfigurácií skriptu.
Výstupné dáta tohto skriptu slúžia pre analýzu metrík nasadenia NEL.
Dáta štruktúrované podľa vyššie uvedenej tabuľky teda filtrujem na len tie zdroje, ktoré sú korektne monitorované technológiou NEL.
Pre nemonitorované alebo nekorektne monitorované (nesprávna konfigurácia) zdroje a domény, si však zaznamenávam ich celkové počty.

\subsubsection{Problémy}

Mesačné dáta HTTP Archive sa v BigQuery vyskytujú rozdelené do dvoch tabuliek (bližšie info k rozdeleniu v sekcií \ref{big-query}):
\begin{enumerate}
    \item tabuľka s dátami z prostredia \code{desktop}, napríklad: \code{2018\_09\_01\_desktop},
    \item tabuľka s dátami z prostredia \code{mobile}, napríklad: \code{2018\_09\_01\_mobile}.
\end{enumerate}

Po konzultácií s pánom Polčákom (vedúci práce a autor predchádzajúcej analýzy) som zistil, že v predchádzajúcej analýze pracovali s týmto rozdelením tak, že obsah tabuliek zlúčili \cite{nel-http-archive}.
Bolo to však robené manuálne, pretože spracovávali vcelku iba 6 mesiacov.
Keďže je počet skúmaných mesiacov v tejto práci oveľa vyšší, rozhodol som sa zlúčenie automatizovať.
Riešenie som zapracoval do použitého príkazu GoogleSQL.

Ďalej, okrem vyššie spomenutého rozdelenia mesačných dát sa v vyskytovali aj mesiace, ktoré boli rozdelené na časti, napríklad:
\begin{enumerate}
    \item \code{2018\_09\_01\_desktop},
    \item \code{2018\_09\_01\_mobile},
    \item \code{2018\_09\_15\_desktop},
    \item \code{2018\_09\_15\_mobile}.
\end{enumerate}

Tento problém som rovnako vyriešil postupným zlučovaním dát z jednotlivých čiastkových tabuliek v príkaze GoogleSQL.
V oboch spomínaných problémoch som rátal s duplicitami dát a preferoval som výber jedinečných záznamov z neskoršieho dátumu za daný mesiac a záznamy z tabuliek \code{desktop} pred tými z tabuliek \code{mobile}.

\subsubsection{Použitie}

% TODO priklad pouzitia na obrázku

\subsection{Skript pre automatizované prehliadanie súčasného webu}

Automatizovaný prehliadač súčasného webu implementuje skript \code{crawl\_and\_store.py}.
Je implementovaný pomocou jazyka Python3 vo verzií \code{3.12.0}. Z knižníc emulujúcich webový 
prehliadač (viď sekciu \ref{selenium}) som si vybral \textit{Playwright}. 
Za pomoci jej rozhrania prehliadam webové stránky na doménach patriacich do zoznamu domén preskúmaných v najaktuálnejších mesačných dát z HTTP Archive.

Pre každú doménu si najskôr vyžiadam domovskú stránku a v prípade úspešnej odpovede z jej obsahu extrahujem všetky dostupné hyperlinky smerujúce na tú istú doménu.
Extrahované hyperlinky udržujem v pomocných objektoch triedy \code{DomainLinkTree}.
Ide o implementáciu stromovej štruktúry pre zoznam dostupných hyperlinkov smerujúcich na aktuálne skúmanú doménu. 
Pri prehliadaní danej domény sa tento strom prehľadáva do šírky (\textit{breadth first search}) aby bol nájdený ďalší hyperlink, ktorý ešte skript nenavštívil.
Najskôr sa teda prehliadajú rozdielne odvetvia stránok (\code{'/home/x'}, \code{'/login/y'}, \code{'/news/z'}) a až potom rôzne zdroje v nich obsiahnuté (\code{'/home/x'}, \code{'/home/y'}, \code{'/home/z'}).
Táto stratégia je doplnená o limitovanie maximálneho počtu hyperlinkov, ktoré má skript preskúmať.
Cieľom takejto implementácie je získať prehľad o využití NEL monitorovania v čo najviac odvetviach stránok na doméne.
Som si však vedomý, že táto stratégia sa nedá uplatniť pre domény, ktoré neštruktúrujú cesty k svojim stránkam do takýchto odvetví --- majú jednoúrovňové cesty k zdrojom.
V takých prípadoch generuje \code{DomainLinkTree} ďalšie hyperlinky podľa poradia, v akom sú pridávané.

Chcené dáta sa získavajú z hlavičiek v HTTP odpovediach z prehliadaných stránok, ale aj z nimi používaných zdrojov ako sú obrázky, skripty a iné.
Tieto dáta sú pre každú doménu separátne ukladané do registra \code{DomainNelDataRegistry}.
Uvedený register predstavuje objekt triedy implementujúcej rozhranie pre prácu so získanými dátami. 
Na pozadí používa \code{DataFrame} objekty z Python knižnice \textit{pandas}, do ktorých dáta priebežne pridáva.
Po dokončení prehliadania každej domény sa z nej získané dáta v registri ukladajú na disk ako Apache Parquet (\code{.parquet}) súbory.
Po dokončení prehliadania poslednej domény sa všetky vytvorené Parquet súbory zlúčia do jedného.
To je docielené tak, že sa všetky načítajú do pamäte skriptu, prepočítajú sa celkové počty spracovaných domén a zdojov a výsledok sa uloží na disk v štruktúre zdieľanej s HTTP Archive skriptom, popísanej tabuľkou \ref{tab:analysis-data-structure}.

\subsubsection{Použitie}
% TODO vyber domen pred pouzitim

\subsection{Skript pre analýzu a produkovanie výsledných metrík}
\subsection{Skripty pre vizualizáciu výsledkov}

% \section{Results}
% \label{sec:results}
